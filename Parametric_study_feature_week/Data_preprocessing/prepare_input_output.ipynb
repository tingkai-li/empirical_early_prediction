{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cells = pd.read_csv(\"../training_V2.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "test_in_cells = pd.read_csv(\"../test_in_V2.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "test_out_cells = pd.read_csv(\"../test_out_V2.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "test_out_cells.remove('G40C3') # This cell has some issued on week 1 rpt, thus omitted\n",
    "# Import interpolated data for knot-point approach and end-to-end\n",
    "Q_data = pd.read_csv(\"../NMC_data_V2_interp_clean.csv\")\n",
    "\n",
    "feature_table_1 = pd.read_csv(\"feature_all_1.csv\")\n",
    "feature_table_2 = pd.read_csv(\"feature_all_2.csv\")\n",
    "feature_table_3 = pd.read_csv(\"feature_all_3.csv\")\n",
    "feature_table_5 = pd.read_csv(\"feature_all_5.csv\")\n",
    "feature_table_6 = pd.read_csv(\"feature_all_6.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty numpy arrays for features and ah-throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_cells = len(training_cells)\n",
    "num_test_in_cells = len(test_in_cells)\n",
    "num_test_out_cells = len(test_out_cells)\n",
    "\n",
    "# Remove non-feature columns from feature list\n",
    "feature_list_1 = feature_table_1.columns.to_list()\n",
    "for element in ['Group', 'Cell','Lifetime']:\n",
    "    feature_list_1.remove(element)\n",
    "feature_list_2 = feature_table_2.columns.to_list()\n",
    "for element in ['Group', 'Cell','Lifetime']:\n",
    "    feature_list_2.remove(element)\n",
    "feature_list_3 = feature_table_3.columns.to_list()\n",
    "for element in ['Group', 'Cell','Lifetime']:\n",
    "    feature_list_3.remove(element)\n",
    "feature_list_5 = feature_table_5.columns.to_list()\n",
    "for element in ['Group', 'Cell','Lifetime']:\n",
    "    feature_list_5.remove(element)\n",
    "feature_list_6 = feature_table_6.columns.to_list()\n",
    "for element in ['Group', 'Cell','Lifetime']:\n",
    "    feature_list_6.remove(element)\n",
    "\n",
    "num_Q = 21\n",
    "num_features = len(feature_list_1)\n",
    "\n",
    "# Create variables for training input as well as training labels for different approaches\n",
    "N_train = np.ndarray((num_training_cells,num_Q))\n",
    "X_train_1 = np.ndarray((num_training_cells,num_features))\n",
    "X_train_2 = np.ndarray((num_training_cells,num_features))\n",
    "X_train_3 = np.ndarray((num_training_cells,num_features))\n",
    "X_train_5 = np.ndarray((num_training_cells,num_features))\n",
    "X_train_6 = np.ndarray((num_training_cells,num_features))\n",
    "\n",
    "# Create variables for test inputs\n",
    "X_test_in_1 = np.ndarray((num_test_in_cells,num_features))\n",
    "X_test_in_2 = np.ndarray((num_test_in_cells,num_features))\n",
    "X_test_in_3 = np.ndarray((num_test_in_cells,num_features))\n",
    "X_test_in_5 = np.ndarray((num_test_in_cells,num_features))\n",
    "X_test_in_6 = np.ndarray((num_test_in_cells,num_features))\n",
    "N_test_in = np.ndarray((num_test_in_cells,num_Q))\n",
    "\n",
    "X_test_out_1 = np.ndarray((num_test_out_cells,num_features))\n",
    "X_test_out_2 = np.ndarray((num_test_out_cells,num_features))\n",
    "X_test_out_3 = np.ndarray((num_test_out_cells,num_features))\n",
    "X_test_out_5 = np.ndarray((num_test_out_cells,num_features))\n",
    "X_test_out_6 = np.ndarray((num_test_out_cells,num_features))\n",
    "N_test_out = np.ndarray((num_test_out_cells,num_Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through training cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iii,cell in enumerate(training_cells):\n",
    "    X_train_1[iii] = feature_table_1[feature_table_1['Cell']==cell][feature_list_1].values\n",
    "    X_train_2[iii] = feature_table_2[feature_table_2['Cell']==cell][feature_list_2].values\n",
    "    X_train_3[iii] = feature_table_3[feature_table_3['Cell']==cell][feature_list_3].values\n",
    "    X_train_5[iii] = feature_table_5[feature_table_5['Cell']==cell][feature_list_5].values\n",
    "    X_train_6[iii] = feature_table_6[feature_table_6['Cell']==cell][feature_list_6].values\n",
    "    \n",
    "    N_cell = Q_data[Q_data['cellID']==cell]['Ah_throughput'].values\n",
    "    N_train[iii] = np.abs(N_cell) # abs() to ensure the first point is nonnegative (very small negative numbers due to interpolation)\n",
    "\n",
    "for iii,cell in enumerate(test_in_cells):\n",
    "    X_test_in_1[iii] = feature_table_1[feature_table_1['Cell']==cell][feature_list_1].values\n",
    "    X_test_in_2[iii] = feature_table_2[feature_table_2['Cell']==cell][feature_list_2].values\n",
    "    X_test_in_3[iii] = feature_table_3[feature_table_3['Cell']==cell][feature_list_3].values\n",
    "    X_test_in_5[iii] = feature_table_5[feature_table_5['Cell']==cell][feature_list_5].values\n",
    "    X_test_in_6[iii] = feature_table_6[feature_table_6['Cell']==cell][feature_list_6].values\n",
    "\n",
    "    N_cell = Q_data[Q_data['cellID']==cell]['Ah_throughput'].values\n",
    "    N_test_in[iii] = np.abs(N_cell) # abs() to ensure the first point is nonnegative (very small negative numbers due to interpolation)\n",
    "\n",
    "for iii,cell in enumerate(test_out_cells):\n",
    "    X_test_out_1[iii] = feature_table_1[feature_table_1['Cell']==cell][feature_list_1].values\n",
    "    X_test_out_2[iii] = feature_table_2[feature_table_2['Cell']==cell][feature_list_2].values\n",
    "    X_test_out_3[iii] = feature_table_3[feature_table_3['Cell']==cell][feature_list_3].values\n",
    "    X_test_out_5[iii] = feature_table_5[feature_table_5['Cell']==cell][feature_list_5].values\n",
    "    X_test_out_6[iii] = feature_table_6[feature_table_6['Cell']==cell][feature_list_6].values\n",
    "\n",
    "    N_cell = Q_data[Q_data['cellID']==cell]['Ah_throughput'].values\n",
    "    N_test_out[iii] = np.abs(N_cell) # abs() to ensure the first point is nonnegative (very small negative numbers due to interpolation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA on features from training set and also transform features from both test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler_1 = StandardScaler()\n",
    "X_train_1_scaled = X_scaler_1.fit_transform(X_train_1)\n",
    "X_test_in_1_scaled = X_scaler_1.transform(X_test_in_1)\n",
    "X_test_out_1_scaled = X_scaler_1.transform(X_test_out_1)\n",
    "PCA_model_1 = PCA(n_components=10)\n",
    "X_train_1_PCA = PCA_model_1.fit_transform(X_train_1_scaled)\n",
    "X_test_in_1_PCA = PCA_model_1.transform(X_test_in_1_scaled)\n",
    "X_test_out_1_PCA = PCA_model_1.transform(X_test_out_1_scaled)\n",
    "\n",
    "X_scaler_2 = StandardScaler()\n",
    "X_train_2_scaled = X_scaler_2.fit_transform(X_train_2)\n",
    "X_test_in_2_scaled = X_scaler_2.transform(X_test_in_2)\n",
    "X_test_out_2_scaled = X_scaler_2.transform(X_test_out_2)\n",
    "PCA_model_2 = PCA(n_components=10)\n",
    "X_train_2_PCA = PCA_model_2.fit_transform(X_train_2_scaled)\n",
    "X_test_in_2_PCA = PCA_model_2.transform(X_test_in_2_scaled)\n",
    "X_test_out_2_PCA = PCA_model_2.transform(X_test_out_2_scaled)\n",
    "\n",
    "X_scaler_3 = StandardScaler()\n",
    "X_train_3_scaled = X_scaler_3.fit_transform(X_train_3)\n",
    "X_test_in_3_scaled = X_scaler_3.transform(X_test_in_3)\n",
    "X_test_out_3_scaled = X_scaler_3.transform(X_test_out_3)\n",
    "PCA_model_3 = PCA(n_components=10)\n",
    "X_train_3_PCA = PCA_model_3.fit_transform(X_train_3_scaled)\n",
    "X_test_in_3_PCA = PCA_model_3.transform(X_test_in_3_scaled)\n",
    "X_test_out_3_PCA = PCA_model_3.transform(X_test_out_3_scaled)\n",
    "\n",
    "X_scaler_5 = StandardScaler()\n",
    "X_train_5_scaled = X_scaler_5.fit_transform(X_train_5)\n",
    "X_test_in_5_scaled = X_scaler_5.transform(X_test_in_5)\n",
    "X_test_out_5_scaled = X_scaler_5.transform(X_test_out_5)\n",
    "PCA_model_5 = PCA(n_components=10)\n",
    "X_train_5_PCA = PCA_model_5.fit_transform(X_train_5_scaled)\n",
    "X_test_in_5_PCA = PCA_model_5.transform(X_test_in_5_scaled)\n",
    "X_test_out_5_PCA = PCA_model_5.transform(X_test_out_5_scaled)\n",
    "\n",
    "X_scaler_6 = StandardScaler()\n",
    "X_train_6_scaled = X_scaler_6.fit_transform(X_train_6)\n",
    "X_test_in_6_scaled = X_scaler_6.transform(X_test_in_6)\n",
    "X_test_out_6_scaled = X_scaler_6.transform(X_test_out_6)\n",
    "PCA_model_6 = PCA(n_components=10)\n",
    "X_train_6_PCA = PCA_model_6.fit_transform(X_train_6_scaled)\n",
    "X_test_in_6_PCA = PCA_model_6.transform(X_test_in_6_scaled)\n",
    "X_test_out_6_PCA = PCA_model_6.transform(X_test_out_6_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"X_train_1_PCA.csv\",X_train_1_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_train_2_PCA.csv\",X_train_2_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_train_3_PCA.csv\",X_train_3_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_train_5_PCA.csv\",X_train_5_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_train_6_PCA.csv\",X_train_6_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_test_in_1_PCA.csv\",X_test_in_1_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_test_in_2_PCA.csv\",X_test_in_2_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_test_in_3_PCA.csv\",X_test_in_3_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_test_in_5_PCA.csv\",X_test_in_5_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_test_in_6_PCA.csv\",X_test_in_6_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_test_out_1_PCA.csv\",X_test_out_1_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_test_out_2_PCA.csv\",X_test_out_2_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_test_out_3_PCA.csv\",X_test_out_3_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_test_out_5_PCA.csv\",X_test_out_5_PCA,delimiter=\",\")\n",
    "np.savetxt(\"X_test_out_6_PCA.csv\",X_test_out_6_PCA,delimiter=\",\")\n",
    "np.savetxt(\"N_train.csv\",N_train,delimiter=\",\")\n",
    "np.savetxt(\"N_test_in.csv\",N_test_in,delimiter=\",\")\n",
    "np.savetxt(\"N_test_out.csv\",N_test_out,delimiter=\",\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
