{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook applying LSTM-based approach proposed by Rieger et al. (2023)\n",
    "\n",
    "Rieger, Laura Hannemose, et al. “Uncertainty-Aware and Explainable Machine Learning for Early Prediction of Battery Degradation Trajectory.” Digital Discovery, vol. 2, no. 1, 2023, pp. 112–22. DOI: https://doi.org/10.1039/D2DD00067A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import KFold,GroupKFold\n",
    "\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset,Subset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the preprocessed training data for the LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_lstm = np.load('Data/train_X_lstm.npy')\n",
    "train_Y_lstm = np.load('Data/train_Y_lstm.npy')\n",
    "train_C_lstm = np.load('Data/train_C_lstm.npy')\n",
    "with open('Data/train_cell_lstm.pkl', 'rb') as f: \n",
    "    train_cell_lstm = pkl.load(f)\n",
    "\n",
    "# Scale the output data to be between -1 and 1, with a minimum value of 0.95 and a maximum value of 1.0\n",
    "min_val = 0.95\n",
    "max_val = 1.0\n",
    "capacity_output_scaler = MinMaxScaler(\n",
    "    (-1, 1),\n",
    "    clip=False).fit(np.maximum(np.minimum(train_Y_lstm[:, 0:1], max_val), min_val))\n",
    "train_Y_lstm[:, 0:1] = capacity_output_scaler.transform(train_Y_lstm[:, 0:1])\n",
    "min_max_range = max_val - min_val\n",
    "# Build the dataset\n",
    "dataset = TensorDataset(torch.tensor(train_X_lstm, dtype=torch.float32),\n",
    "                        torch.tensor(train_Y_lstm, dtype=torch.float32),\n",
    "                        torch.tensor(train_C_lstm, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_Y_lstm[:, 0:1], bins=np.arange(-1, 1, 0.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the LSTM model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Uncertain_LSTM_new(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_in=1,\n",
    "        num_augment=3,\n",
    "        num_hidden=100,\n",
    "        seq_len=5,\n",
    "        n_layers=2,\n",
    "        dropout=0.0,\n",
    "        num_hidden_lstm=-1,\n",
    "    ):\n",
    "        super(Uncertain_LSTM_new, self).__init__()\n",
    "        self.hidden_dim = num_hidden\n",
    "        if num_hidden_lstm == -1:\n",
    "            num_hidden_lstm = num_hidden\n",
    "        self.hidden_dim_lstm = num_hidden_lstm\n",
    "        self.seq_len = seq_len\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            num_in, self.hidden_dim_lstm, num_layers=n_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.linear1 = nn.Linear(self.hidden_dim_lstm + num_augment, num_hidden)\n",
    "        self.mean_state_layer = nn.Linear(num_hidden, 1)\n",
    "        self.var_state_layer = nn.Linear(num_hidden, 1)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.drop_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initializes the weights of the network.\"\"\"\n",
    "        # Initialize LSTM weights\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param)  # Xavier initialization for input weights\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)  # Orthogonal initialization for hidden state weights\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)  # Zero initialize biases\n",
    "\n",
    "        # Initialize fully connected layers\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        self.linear1.bias.data.fill_(0)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.mean_state_layer.weight)\n",
    "        self.mean_state_layer.bias.data.fill_(0)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.var_state_layer.weight)\n",
    "        self.var_state_layer.bias.data.fill_(-2.0)  # Helps keep variance low initially\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x, _ = self.lstm1(x)\n",
    "\n",
    "        x = x[:, -1].view(-1, self.hidden_dim_lstm)\n",
    "\n",
    "        x = torch.cat((x, c), dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.drop_layer(x)\n",
    "\n",
    "        mean_state = torch.tanh(self.mean_state_layer(x))\n",
    "        var_state = F.softplus(self.var_state_layer(x)) + 1e-6\n",
    "\n",
    "        return mean_state, var_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the NLL loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(target, mean, var):\n",
    "    # print(target.shape, mean.shape, var.shape)\n",
    "    nll = 0.5 * (torch.log(var) + torch.square(target - mean) / ((var)))\n",
    "    return torch.sum(nll)\n",
    "\n",
    "def mse_loss(target, mean):\n",
    "    return 0.5*torch.sum(torch.square(target - mean))\n",
    "\n",
    "# Define the NLL loss function with warmup (i.e., ignore the variance)\n",
    "def nll_loss_warmup(target, mean, var, warmup=False):\n",
    "    if warmup:\n",
    "        # Ignore the variance resulting in a simple MSE loss\n",
    "        var_log = 0\n",
    "        var = 1\n",
    "        nll = 0.5 * (var_log + torch.square(target - mean) / var)\n",
    "    else:\n",
    "        nll = 0.5 * (torch.log(var) + torch.square(target - mean) / var)\n",
    "    return torch.sum(nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for LSTM model\n",
    "batch_size = 128\n",
    "max_epochs = 1000\n",
    "lr = 1e-3\n",
    "hidden_size = 16\n",
    "hidden_size_lstm = 32\n",
    "num_layers = 2\n",
    "cur_patience = 0\n",
    "max_patience = 5\n",
    "patience_delta = 0\n",
    "dropout = 0.0\n",
    "\n",
    "# Additional configurations\n",
    "sequence_length = 10\n",
    "num_augment = train_C_lstm.shape[1]\n",
    "input_size = train_X_lstm.shape[2]\n",
    "num_folds = 10\n",
    "num_ensemble = 5\n",
    "warmup_epochs = 10 # Number of epochs to ignore the variance\n",
    "stabilize_epochs = 10 # Number of epochs to stabilize the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ensemble of 5 models, 10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "kf = GroupKFold(n_splits=num_folds)\n",
    "kfold = kf.split(train_X_lstm, groups=train_cell_lstm)\n",
    "model_state_dict = {}\n",
    "for fold,(train_idx,val_idx) in enumerate(kfold):\n",
    "    model_state_dict[fold] = []\n",
    "    print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "    # Subset the TensorDataset for training and validation\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    # Create the DataLoader for training and validation\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for i in range(num_ensemble):\n",
    "        print(f\"Fold {fold+1}: Ensemble {i+1}/{num_ensemble}\")\n",
    "        # Initialize the model\n",
    "        model = Uncertain_LSTM_new(\n",
    "            num_in=input_size,\n",
    "            num_augment=num_augment,\n",
    "            num_hidden=hidden_size,\n",
    "            seq_len=sequence_length,\n",
    "            n_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            num_hidden_lstm=hidden_size_lstm,\n",
    "        )\n",
    "        # Initialize the warmup flag\n",
    "        warmup = True\n",
    "        for param in model.var_state_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Initialize the loss function\n",
    "        criterion = nll_loss_warmup\n",
    "\n",
    "        # Initialize the early stopping variables\n",
    "        best_loss = np.inf\n",
    "        cur_patience = 0\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            tr_loss = 0\n",
    "            tr_loss_mse = 0\n",
    "            # Reset warmup flag after warmup_epochs\n",
    "            if epoch == warmup_epochs:\n",
    "                warmup = False\n",
    "                for param in model.var_state_layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            for i, (x, y, c) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                mean, var = model(x, c)\n",
    "                loss = criterion(y[:,0], mean[:,0], var[:,0], warmup)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tr_loss += loss.item()\n",
    "                tr_loss_mse += mse_loss(y[:,0], mean[:,0]).item()\n",
    "\n",
    "            tr_loss /= len(train_loader.dataset)\n",
    "            tr_loss_mse /= len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                val_loss_mse = 0\n",
    "                for i, (x_val, y_val, c_val) in enumerate(val_loader):\n",
    "                    mean_val, var_val = model(x_val, c_val)\n",
    "                    val_loss += criterion(y_val[:,0], mean_val[:,0], var_val[:,0], warmup).item()\n",
    "                    val_loss_mse += mse_loss(y_val[:,0], mean_val[:,0]).item()\n",
    "                val_loss /= len(val_loader.dataset)\n",
    "                val_loss_mse /= len(val_loader.dataset)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{max_epochs}, Train Loss: {tr_loss:.4f}, Val Loss: {val_loss:.4f}, Train MSE: {tr_loss_mse:.4f}, Val MSE: {val_loss_mse:.4f}\")\n",
    "\n",
    "                if (\n",
    "                        val_loss + patience_delta < best_loss\n",
    "                        and epoch > warmup_epochs + stabilize_epochs\n",
    "                    ): \n",
    "                    best_loss = val_loss\n",
    "                    best_model = model.state_dict()\n",
    "                    cur_patience = 0\n",
    "                else:\n",
    "                    if epoch > warmup_epochs + stabilize_epochs:\n",
    "                        cur_patience += 1\n",
    "                        if cur_patience >= max_patience:\n",
    "                            break\n",
    "\n",
    "        model_state_dict[fold].append(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_state_dict.pth', 'wb') as f:\n",
    "    torch.save(model_state_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model state dictionary\n",
    "with open('model_state_dict.pth', 'rb') as f:\n",
    "    model_state_dict = torch.load(f)\n",
    "\n",
    "training_cells = pd.read_csv(\"../Data_preprocessing/training.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "test_in_cells = pd.read_csv(\"../Data_preprocessing/test_in.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "test_out_cells = pd.read_csv(\"../Data_preprocessing/test_out.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "\n",
    "# Load the preprocessed data for evaluation\n",
    "train_X_lstm_eval = np.load('Data/train_X_lstm_eval.npy')\n",
    "with open('Data/train_remaining_eval.pkl', 'rb') as f:\n",
    "    train_remaining_eval = pkl.load(f)\n",
    "with open('Data/train_C_lstm_eval.pkl', 'rb') as f:\n",
    "    train_C_lstm_eval = pkl.load(f)\n",
    "\n",
    "test_in_X_lstm_eval = np.load('Data/test_in_X_lstm_eval.npy')\n",
    "with open('Data/test_in_remaining_eval.pkl', 'rb') as f:\n",
    "    test_in_remaining_eval = pkl.load(f)\n",
    "with open('Data/test_in_C_lstm_eval.pkl', 'rb') as f:\n",
    "    test_in_C_lstm_eval = pkl.load(f)\n",
    "\n",
    "\n",
    "test_out_X_lstm_eval = np.load('Data/test_out_X_lstm_eval.npy')\n",
    "with open('Data/test_out_remaining_eval.pkl', 'rb') as f:\n",
    "    test_out_remaining_eval = pkl.load(f)\n",
    "with open('Data/test_out_C_lstm_eval.pkl', 'rb') as f:\n",
    "    test_out_C_lstm_eval = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Ensemble 1/5\n",
      "Fold 1: Ensemble 2/5\n",
      "Fold 1: Ensemble 3/5\n",
      "Fold 1: Ensemble 4/5\n",
      "Fold 1: Ensemble 5/5\n",
      "Fold 2: Ensemble 1/5\n",
      "Fold 2: Ensemble 2/5\n",
      "Fold 2: Ensemble 3/5\n",
      "Fold 2: Ensemble 4/5\n",
      "Fold 2: Ensemble 5/5\n",
      "Fold 3: Ensemble 1/5\n",
      "Fold 3: Ensemble 2/5\n",
      "Fold 3: Ensemble 3/5\n",
      "Fold 3: Ensemble 4/5\n",
      "Fold 3: Ensemble 5/5\n",
      "Fold 4: Ensemble 1/5\n",
      "Fold 4: Ensemble 2/5\n",
      "Fold 4: Ensemble 3/5\n",
      "Fold 4: Ensemble 4/5\n",
      "Fold 4: Ensemble 5/5\n",
      "Fold 5: Ensemble 1/5\n",
      "Fold 5: Ensemble 2/5\n",
      "Fold 5: Ensemble 3/5\n",
      "Fold 5: Ensemble 4/5\n",
      "Fold 5: Ensemble 5/5\n",
      "Fold 6: Ensemble 1/5\n",
      "Fold 6: Ensemble 2/5\n",
      "Fold 6: Ensemble 3/5\n",
      "Fold 6: Ensemble 4/5\n",
      "Fold 6: Ensemble 5/5\n",
      "Fold 7: Ensemble 1/5\n",
      "Fold 7: Ensemble 2/5\n",
      "Fold 7: Ensemble 3/5\n",
      "Fold 7: Ensemble 4/5\n",
      "Fold 7: Ensemble 5/5\n",
      "Fold 8: Ensemble 1/5\n",
      "Fold 8: Ensemble 2/5\n",
      "Fold 8: Ensemble 3/5\n",
      "Fold 8: Ensemble 4/5\n",
      "Fold 8: Ensemble 5/5\n",
      "Fold 9: Ensemble 1/5\n",
      "Fold 9: Ensemble 2/5\n",
      "Fold 9: Ensemble 3/5\n",
      "Fold 9: Ensemble 4/5\n",
      "Fold 9: Ensemble 5/5\n",
      "Fold 10: Ensemble 1/5\n",
      "Fold 10: Ensemble 2/5\n",
      "Fold 10: Ensemble 3/5\n",
      "Fold 10: Ensemble 4/5\n",
      "Fold 10: Ensemble 5/5\n"
     ]
    }
   ],
   "source": [
    "Q_train_true = {}\n",
    "Q_test_in_true = {}\n",
    "Q_test_out_true = {}\n",
    "\n",
    "Q_train_pred_5_all = {}\n",
    "Q_test_in_pred_5_all = {}\n",
    "Q_test_out_pred_5_all = {}\n",
    "\n",
    "std_train_pred_5_all = {}\n",
    "std_test_in_pred_5_all = {}\n",
    "std_test_out_pred_5_all = {}\n",
    "\n",
    "Q_train_pred_5_ensemble = {}\n",
    "Q_test_in_pred_5_ensemble = {}\n",
    "Q_test_out_pred_5_ensemble = {}\n",
    "\n",
    "std_train_pred_5_ensemble = {}\n",
    "std_test_in_pred_5_ensemble = {}\n",
    "std_test_out_pred_5_ensemble = {}\n",
    "\n",
    "\n",
    "all_pred_samples = {}\n",
    "\n",
    "for fold in range(10):\n",
    "    Q_train_pred_5_all[fold] = {}\n",
    "    std_train_pred_5_all[fold] = {}\n",
    "    all_pred_samples[fold] = []\n",
    "    for i in range(5):\n",
    "        print(f\"Fold {fold+1}: Ensemble {i+1}/{num_ensemble}\")\n",
    "        # Initialize the model\n",
    "        model = Uncertain_LSTM_new(\n",
    "            num_in=input_size,\n",
    "            num_augment=num_augment,\n",
    "            num_hidden=hidden_size,\n",
    "            seq_len=sequence_length,\n",
    "            n_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            num_hidden_lstm=hidden_size_lstm,\n",
    "        )\n",
    "        # Load the trained model parameters\n",
    "        model.load_state_dict(model_state_dict[fold][i])\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Evaluate the model on the training set\n",
    "            for j in range(train_X_lstm_eval.shape[0]):\n",
    "                # Indexing the cell\n",
    "                cell = training_cells[j]\n",
    "                if cell not in Q_train_pred_5_all[fold]:\n",
    "                    Q_train_pred_5_all[fold][cell] = []\n",
    "                    std_train_pred_5_all[fold][cell] = []\n",
    "                \n",
    "                # Get the observed capacity\n",
    "                x = torch.tensor(train_X_lstm_eval[j:j+1], dtype=torch.float32)\n",
    "                x_pred = x.clone()\n",
    "                # Get predicted remaining capacity\n",
    "                Q_pred = []\n",
    "                std_pred = []\n",
    "                for k in range(len(train_remaining_eval[cell])):\n",
    "                    # Find the features with the corresponding Ah-throughput location\n",
    "                    c = torch.tensor(train_C_lstm_eval[cell][k].reshape(1,-1), dtype=torch.float32)\n",
    "                    # Predict the remaining capacity ratio and variance\n",
    "                    seq = x_pred[:,-10:,:]\n",
    "                    mean_scaled, var_scaled = model(seq, c) # outputs are (ratio between existing and new capacity, log variance)\n",
    "                    # Squeeze the tensor to remove the batch dimension\n",
    "                    mean_scaled = mean_scaled.squeeze()\n",
    "                    all_pred_samples[fold].append(mean_scaled.item())\n",
    "                    var_scaled = var_scaled.squeeze()\n",
    "                    # Inverse transform the scaled ratio to the original scale\n",
    "                    var_ratio = var_scaled*(min_max_range**2)\n",
    "                    ratio = torch.tensor(capacity_output_scaler.inverse_transform(np.array(mean_scaled.item()).reshape(-1,1)), dtype=torch.float32)\n",
    "\n",
    "                    # Append the predicted remaining capacity ratio\n",
    "                    x_pred = torch.cat((x_pred, ratio[:]*(x_pred[:,-1,-1])[None,:,None]), dim=1)\n",
    "\n",
    "                    # Append the predicted standard deviation\n",
    "                    std_pred.append(np.sqrt(var_ratio.item())*(x_pred[:,-1,-1])[None,:,None].item())  \n",
    "\n",
    "                # Concatenate the true remaining capacity with the observed capacity\n",
    "                x_true = torch.concat((x, torch.tensor(train_remaining_eval[cell], dtype=torch.float32)[None,:,None]), dim=1)\n",
    "                if cell not in Q_train_true:\n",
    "                    Q_train_true[cell] = x_true.squeeze().numpy()\n",
    "                Q_train_pred_5_all[fold][cell].append(x_pred.squeeze().numpy())\n",
    "                std_train_pred_5_all[fold][cell].append(np.array(std_pred))\n",
    "\n",
    "with open('Q_train_pred_5_all.pkl', 'wb') as f:\n",
    "    pkl.dump(Q_train_pred_5_all, f)\n",
    "\n",
    "with open('std_train_pred_5_all.pkl', 'wb') as f:\n",
    "    pkl.dump(std_train_pred_5_all, f)\n",
    "\n",
    "with open('Q_train_true.pkl', 'wb') as f:\n",
    "    pkl.dump(Q_train_true, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Ensemble 1/5\n",
      "Fold 1: Ensemble 2/5\n",
      "Fold 1: Ensemble 3/5\n",
      "Fold 1: Ensemble 4/5\n",
      "Fold 1: Ensemble 5/5\n",
      "Fold 2: Ensemble 1/5\n",
      "Fold 2: Ensemble 2/5\n",
      "Fold 2: Ensemble 3/5\n",
      "Fold 2: Ensemble 4/5\n",
      "Fold 2: Ensemble 5/5\n",
      "Fold 3: Ensemble 1/5\n",
      "Fold 3: Ensemble 2/5\n",
      "Fold 3: Ensemble 3/5\n",
      "Fold 3: Ensemble 4/5\n",
      "Fold 3: Ensemble 5/5\n",
      "Fold 4: Ensemble 1/5\n",
      "Fold 4: Ensemble 2/5\n",
      "Fold 4: Ensemble 3/5\n",
      "Fold 4: Ensemble 4/5\n",
      "Fold 4: Ensemble 5/5\n",
      "Fold 5: Ensemble 1/5\n",
      "Fold 5: Ensemble 2/5\n",
      "Fold 5: Ensemble 3/5\n",
      "Fold 5: Ensemble 4/5\n",
      "Fold 5: Ensemble 5/5\n",
      "Fold 6: Ensemble 1/5\n",
      "Fold 6: Ensemble 2/5\n",
      "Fold 6: Ensemble 3/5\n",
      "Fold 6: Ensemble 4/5\n",
      "Fold 6: Ensemble 5/5\n",
      "Fold 7: Ensemble 1/5\n",
      "Fold 7: Ensemble 2/5\n",
      "Fold 7: Ensemble 3/5\n",
      "Fold 7: Ensemble 4/5\n",
      "Fold 7: Ensemble 5/5\n",
      "Fold 8: Ensemble 1/5\n",
      "Fold 8: Ensemble 2/5\n",
      "Fold 8: Ensemble 3/5\n",
      "Fold 8: Ensemble 4/5\n",
      "Fold 8: Ensemble 5/5\n",
      "Fold 9: Ensemble 1/5\n",
      "Fold 9: Ensemble 2/5\n",
      "Fold 9: Ensemble 3/5\n",
      "Fold 9: Ensemble 4/5\n",
      "Fold 9: Ensemble 5/5\n",
      "Fold 10: Ensemble 1/5\n",
      "Fold 10: Ensemble 2/5\n",
      "Fold 10: Ensemble 3/5\n",
      "Fold 10: Ensemble 4/5\n",
      "Fold 10: Ensemble 5/5\n"
     ]
    }
   ],
   "source": [
    "for fold in range(10):\n",
    "    Q_test_in_pred_5_all[fold] = {}\n",
    "    std_test_in_pred_5_all[fold] = {}\n",
    "    all_pred_samples[fold] = []\n",
    "    for i in range(5):\n",
    "        print(f\"Fold {fold+1}: Ensemble {i+1}/{num_ensemble}\")\n",
    "        # Initialize the model\n",
    "        model = Uncertain_LSTM_new(\n",
    "            num_in=input_size,\n",
    "            num_augment=num_augment,\n",
    "            num_hidden=hidden_size,\n",
    "            seq_len=sequence_length,\n",
    "            n_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            num_hidden_lstm=hidden_size_lstm,\n",
    "        )\n",
    "        # Load the trained model parameters\n",
    "        model.load_state_dict(model_state_dict[fold][i])\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Evaluate the model on the training set\n",
    "            for j in range(test_in_X_lstm_eval.shape[0]):\n",
    "                # Indexing the cell\n",
    "                cell = test_in_cells[j]\n",
    "                if cell not in Q_test_in_pred_5_all[fold]:\n",
    "                    Q_test_in_pred_5_all[fold][cell] = []\n",
    "                    std_test_in_pred_5_all[fold][cell] = []\n",
    "                \n",
    "                # Get the observed capacity\n",
    "                x = torch.tensor(test_in_X_lstm_eval[j:j+1], dtype=torch.float32)\n",
    "                x_pred = x.clone()\n",
    "                # Get predicted remaining capacity\n",
    "                Q_pred = []\n",
    "                std_pred = []\n",
    "                for k in range(len(test_in_remaining_eval[cell])):\n",
    "                    # Find the features with the corresponding Ah-throughput location\n",
    "                    c = torch.tensor(test_in_C_lstm_eval[cell][k].reshape(1,-1), dtype=torch.float32)\n",
    "                    # Predict the remaining capacity ratio and variance\n",
    "                    seq = x_pred[:,-10:,:]\n",
    "                    mean_scaled, var_scaled = model(seq, c) # outputs are (ratio between existing and new capacity, log variance)\n",
    "                    # Squeeze the tensor to remove the batch dimension\n",
    "                    mean_scaled = mean_scaled.squeeze()\n",
    "                    all_pred_samples[fold].append(mean_scaled.item())\n",
    "                    var_scaled = var_scaled.squeeze()\n",
    "                    # Inverse transform the scaled ratio to the original scale\n",
    "                    ratio = torch.tensor(capacity_output_scaler.inverse_transform(np.array(mean_scaled.item()).reshape(-1,1)), dtype=torch.float32)\n",
    "                    var_ratio = var_scaled*(min_max_range**2)\n",
    "                    # Append the predicted remaining capacity ratio\n",
    "                    x_pred = torch.cat((x_pred, ratio[:]*(x_pred[:,-1,-1])[None,:,None]), dim=1)\n",
    "                    \n",
    "                    # Append the predicted standard deviation\n",
    "                    std_pred.append(np.sqrt(var_ratio.item())*(x_pred[:,-1,-1])[None,:,None].item())  \n",
    "\n",
    "                # Concatenate the true remaining capacity with the observed capacity\n",
    "                x_true = torch.concat((x, torch.tensor(test_in_remaining_eval[cell], dtype=torch.float32)[None,:,None]), dim=1)\n",
    "                if cell not in Q_test_in_true:\n",
    "                    Q_test_in_true[cell] = x_true.squeeze().numpy()\n",
    "                Q_test_in_pred_5_all[fold][cell].append(x_pred.squeeze().numpy())\n",
    "                std_test_in_pred_5_all[fold][cell].append(np.array(std_pred))\n",
    "\n",
    "with open('Results/Q_test_in_pred_5_all.pkl', 'wb') as f:\n",
    "    pkl.dump(Q_test_in_pred_5_all, f)\n",
    "\n",
    "with open('Results/std_test_in_pred_5_all.pkl', 'wb') as f:\n",
    "    pkl.dump(std_test_in_pred_5_all, f)\n",
    "\n",
    "with open('Results/Q_test_in_true.pkl', 'wb') as f:\n",
    "    pkl.dump(Q_test_in_true, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Ensemble 1/5\n",
      "Fold 1: Ensemble 2/5\n",
      "Fold 1: Ensemble 3/5\n",
      "Fold 1: Ensemble 4/5\n",
      "Fold 1: Ensemble 5/5\n",
      "Fold 2: Ensemble 1/5\n",
      "Fold 2: Ensemble 2/5\n",
      "Fold 2: Ensemble 3/5\n",
      "Fold 2: Ensemble 4/5\n",
      "Fold 2: Ensemble 5/5\n",
      "Fold 3: Ensemble 1/5\n",
      "Fold 3: Ensemble 2/5\n",
      "Fold 3: Ensemble 3/5\n",
      "Fold 3: Ensemble 4/5\n",
      "Fold 3: Ensemble 5/5\n",
      "Fold 4: Ensemble 1/5\n",
      "Fold 4: Ensemble 2/5\n",
      "Fold 4: Ensemble 3/5\n",
      "Fold 4: Ensemble 4/5\n",
      "Fold 4: Ensemble 5/5\n",
      "Fold 5: Ensemble 1/5\n",
      "Fold 5: Ensemble 2/5\n",
      "Fold 5: Ensemble 3/5\n",
      "Fold 5: Ensemble 4/5\n",
      "Fold 5: Ensemble 5/5\n",
      "Fold 6: Ensemble 1/5\n",
      "Fold 6: Ensemble 2/5\n",
      "Fold 6: Ensemble 3/5\n",
      "Fold 6: Ensemble 4/5\n",
      "Fold 6: Ensemble 5/5\n",
      "Fold 7: Ensemble 1/5\n",
      "Fold 7: Ensemble 2/5\n",
      "Fold 7: Ensemble 3/5\n",
      "Fold 7: Ensemble 4/5\n",
      "Fold 7: Ensemble 5/5\n",
      "Fold 8: Ensemble 1/5\n",
      "Fold 8: Ensemble 2/5\n",
      "Fold 8: Ensemble 3/5\n",
      "Fold 8: Ensemble 4/5\n",
      "Fold 8: Ensemble 5/5\n",
      "Fold 9: Ensemble 1/5\n",
      "Fold 9: Ensemble 2/5\n",
      "Fold 9: Ensemble 3/5\n",
      "Fold 9: Ensemble 4/5\n",
      "Fold 9: Ensemble 5/5\n",
      "Fold 10: Ensemble 1/5\n",
      "Fold 10: Ensemble 2/5\n",
      "Fold 10: Ensemble 3/5\n",
      "Fold 10: Ensemble 4/5\n",
      "Fold 10: Ensemble 5/5\n"
     ]
    }
   ],
   "source": [
    "for fold in range(10):\n",
    "    Q_test_out_pred_5_all[fold] = {}\n",
    "    std_test_out_pred_5_all[fold] = {}\n",
    "    for i in range(5):\n",
    "        print(f\"Fold {fold+1}: Ensemble {i+1}/{num_ensemble}\")\n",
    "        # Initialize the model\n",
    "        model = Uncertain_LSTM_new(\n",
    "            num_in=input_size,\n",
    "            num_augment=num_augment,\n",
    "            num_hidden=hidden_size,\n",
    "            seq_len=sequence_length,\n",
    "            n_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            num_hidden_lstm=hidden_size_lstm,\n",
    "        )\n",
    "        # Load the trained model parameters\n",
    "        model.load_state_dict(model_state_dict[fold][i])\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Evaluate the model on the training set\n",
    "            for j in range(test_out_X_lstm_eval.shape[0]):\n",
    "                # Indexing the cell\n",
    "                cell = test_out_cells[j]\n",
    "                if cell not in Q_test_out_pred_5_all[fold]:\n",
    "                    Q_test_out_pred_5_all[fold][cell] = []\n",
    "                    std_test_out_pred_5_all[fold][cell] = []\n",
    "                \n",
    "                # Get the observed capacity\n",
    "                x = torch.tensor(test_out_X_lstm_eval[j:j+1], dtype=torch.float32)\n",
    "                x_pred = x.clone()\n",
    "                # Get predicted remaining capacity\n",
    "                Q_pred = []\n",
    "                std_pred = []\n",
    "                for k in range(len(test_out_remaining_eval[cell])):\n",
    "                    # Find the features with the corresponding Ah-throughput location\n",
    "                    c = torch.tensor(test_out_C_lstm_eval[cell][k].reshape(1,-1), dtype=torch.float32)\n",
    "                    # Predict the remaining capacity ratio and variance\n",
    "                    seq = x_pred[:,-10:,:]\n",
    "                    mean_scaled, var_scaled = model(seq, c) # outputs are (ratio between existing and new capacity,  variance)\n",
    "                    # Squeeze the tensor to remove the batch dimension\n",
    "                    mean_scaled = mean_scaled.squeeze()\n",
    "                    all_pred_samples[fold].append(mean_scaled.item())\n",
    "                    var_scaled = var_scaled.squeeze()\n",
    "                    # Inverse transform the scaled ratio to the original scale\n",
    "                    ratio = torch.tensor(capacity_output_scaler.inverse_transform(np.array(mean_scaled.item()).reshape(-1,1)), dtype=torch.float32)\n",
    "                    var_ratio = var_scaled*(min_max_range**2)\n",
    "                    # Append the predicted remaining capacity ratio\n",
    "                    x_pred = torch.cat((x_pred, ratio[:]*(x_pred[:,-1,-1])[None,:,None]), dim=1)\n",
    "                    # Append the predicted standard deviation\n",
    "                    std_pred.append(np.sqrt(var_ratio.item())*(x_pred[:,-1,-1])[None,:,None].item())\n",
    "\n",
    "                # Concatenate the true remaining capacity with the observed capacity\n",
    "                x_true = torch.concat((x, torch.tensor(test_out_remaining_eval[cell], dtype=torch.float32)[None,:,None]), dim=1)\n",
    "                if cell not in Q_test_out_true:\n",
    "                    Q_test_out_true[cell] = x_true.squeeze().numpy()\n",
    "                Q_test_out_pred_5_all[fold][cell].append(x_pred.squeeze().numpy())\n",
    "                std_test_out_pred_5_all[fold][cell].append(np.array(std_pred))\n",
    "\n",
    "with open('Results/Q_test_out_pred_5_all.pkl', 'wb') as f:\n",
    "    pkl.dump(Q_test_out_pred_5_all, f)\n",
    "\n",
    "with open('Results/std_test_out_pred_5_all.pkl', 'wb') as f:\n",
    "    pkl.dump(std_test_out_pred_5_all, f)\n",
    "\n",
    "with open('Results/Q_test_out_true.pkl', 'wb') as f:\n",
    "    pkl.dump(Q_test_out_true, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble the predictions\n",
    "for fold in range(10):\n",
    "    Q_train_pred_5_ensemble[fold] = {}\n",
    "    std_train_pred_5_ensemble[fold] = {}\n",
    "    for cell in training_cells:\n",
    "        Q_train_pred_5_all[fold][cell] = np.array(Q_train_pred_5_all[fold][cell])\n",
    "        Q_train_pred_5_ensemble[fold][cell] = np.mean(Q_train_pred_5_all[fold][cell], axis=0)[10:]\n",
    "        std_train_pred_5_ensemble[fold][cell] = np.sqrt(\n",
    "            np.mean(np.square(std_train_pred_5_all[fold][cell])\n",
    "                    +Q_train_pred_5_all[fold][cell][:,10:]**2-Q_train_pred_5_ensemble[fold][cell]**2, axis=0))\n",
    "        \n",
    "    Q_test_in_pred_5_ensemble[fold] = {}\n",
    "    std_test_in_pred_5_ensemble[fold] = {}\n",
    "    for cell in test_in_cells:\n",
    "        Q_test_in_pred_5_all[fold][cell] = np.array(Q_test_in_pred_5_all[fold][cell])\n",
    "        Q_test_in_pred_5_ensemble[fold][cell] = np.mean(Q_test_in_pred_5_all[fold][cell], axis=0)[10:]\n",
    "        std_test_in_pred_5_ensemble[fold][cell] = np.sqrt(\n",
    "            np.mean(np.square(std_test_in_pred_5_all[fold][cell])\n",
    "                    +Q_test_in_pred_5_all[fold][cell][:,10:]**2-Q_test_in_pred_5_ensemble[fold][cell]**2, axis=0))\n",
    "        \n",
    "    Q_test_out_pred_5_ensemble[fold] = {}\n",
    "    std_test_out_pred_5_ensemble[fold] = {}\n",
    "    for cell in test_out_cells:\n",
    "        Q_test_out_pred_5_all[fold][cell] = np.array(Q_test_out_pred_5_all[fold][cell])\n",
    "        Q_test_out_pred_5_ensemble[fold][cell] = np.mean(Q_test_out_pred_5_all[fold][cell], axis=0)[10:]\n",
    "        std_test_out_pred_5_ensemble[fold][cell] = np.sqrt(\n",
    "            np.mean(np.square(std_test_out_pred_5_all[fold][cell])\n",
    "                    +Q_test_out_pred_5_all[fold][cell][:,10:]**2-Q_test_out_pred_5_ensemble[fold][cell]**2, axis=0))\n",
    "        \n",
    "with open('Results/Q_train_pred_5_ensemble.pkl', 'wb') as f:\n",
    "    pkl.dump(Q_train_pred_5_ensemble, f)\n",
    "\n",
    "with open('Results/std_train_pred_5_ensemble.pkl', 'wb') as f:\n",
    "    pkl.dump(std_train_pred_5_ensemble, f)\n",
    "\n",
    "with open('Results/Q_test_in_pred_5_ensemble.pkl', 'wb') as f:\n",
    "    pkl.dump(Q_test_in_pred_5_ensemble, f)\n",
    "\n",
    "with open('Results/std_test_in_pred_5_ensemble.pkl', 'wb') as f:\n",
    "    pkl.dump(std_test_in_pred_5_ensemble, f)\n",
    "\n",
    "with open('Results/Q_test_out_pred_5_ensemble.pkl', 'wb') as f:\n",
    "    pkl.dump(Q_test_out_pred_5_ensemble, f)\n",
    "\n",
    "with open('Results/std_test_out_pred_5_ensemble.pkl', 'wb') as f:\n",
    "    pkl.dump(std_test_out_pred_5_ensemble, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
