{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract PCA features and training labels\n",
    "Loop through cells to ensure input-output are pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load cell lists, interpolated capacity data, and fitted empirical parameters on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cells = pd.read_csv(\"training_V2.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "test_in_cells = pd.read_csv(\"test_in_V2.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "test_out_cells = pd.read_csv(\"test_out_V2.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "\n",
    "# Import interpolated data for knot-point approach and end-to-end\n",
    "Q_data = pd.read_csv(\"NMC_data_V2_interp_clean.csv\")\n",
    "# Import fitted parameters for sequential optimization\n",
    "# b_data = pd.read_csv(\"empirical_parameters_train_final.csv\")\n",
    "b_data = pd.read_csv(\"Empirical_parameters_train_py.csv\",index_col=0)\n",
    "\n",
    "# Import feature table\n",
    "feature_table = pd.read_csv(\"feature_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create numpy array holders for all needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_cells = len(training_cells)\n",
    "num_test_in_cells = len(test_in_cells)\n",
    "num_test_out_cells = len(test_out_cells)\n",
    "\n",
    "feature_list = feature_table.columns.to_list()\n",
    "for element in ['Group', 'Cell','Lifetime']:\n",
    "    feature_list.remove(element)\n",
    "\n",
    "num_parameters = 3\n",
    "num_knot = 5\n",
    "num_Q = 21\n",
    "num_features = len(feature_list)\n",
    "\n",
    "# Create variables for training input as well as training labels for different approaches\n",
    "b_train = np.ndarray((num_training_cells,num_parameters))\n",
    "N_train = np.ndarray((num_training_cells,num_Q))\n",
    "\n",
    "knot_train = np.ndarray((num_training_cells,num_knot))\n",
    "X_train = np.ndarray((num_training_cells,num_features))\n",
    "\n",
    "# Create variables for test inputs\n",
    "X_test_in = np.ndarray((num_test_in_cells,num_features))\n",
    "N_test_in = np.ndarray((num_test_in_cells,num_Q))\n",
    "\n",
    "X_test_out = np.ndarray((num_test_out_cells,num_features))\n",
    "N_test_out = np.ndarray((num_test_out_cells,num_Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iii,cell in enumerate(training_cells):\n",
    "    feature_cell = feature_table[feature_table['Cell']==cell][feature_list].values\n",
    "    b_cell = b_data[b_data['Cell']==cell][['b1','b2','b3']].values\n",
    "    N_cell = Q_data[Q_data['cellID']==cell]['Ah_throughput'].values\n",
    "    X_train[iii] = feature_cell\n",
    "    b_train[iii] = b_cell\n",
    "    N_train[iii] = np.abs(N_cell) # abs() to ensure the first point is nonnegative (very small negative numbers due to interpolation)\n",
    "    \n",
    "    knot_position = Q_data[Q_data['cellID']==cell]['Ah_throughput'].to_numpy()[0:21:4]\n",
    "    knot_pos_diff = np.diff(knot_position)\n",
    "    knot_train[iii] = knot_pos_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through high-DoD test data (test_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iii,cell in enumerate(test_in_cells):\n",
    "    feature_cell = feature_table[feature_table['Cell']==cell][feature_list].values\n",
    "    N_cell = Q_data[Q_data['cellID']==cell]['Ah_throughput'].values\n",
    "\n",
    "    X_test_in[iii] = feature_cell\n",
    "    N_test_in[iii] = np.abs(N_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through low-DoD test data (test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iii,cell in enumerate(test_out_cells):\n",
    "    feature_cell = feature_table[feature_table['Cell']==cell][feature_list].values\n",
    "    N_cell = Q_data[Q_data['cellID']==cell]['Ah_throughput'].values\n",
    "\n",
    "    X_test_out[iii] = feature_cell\n",
    "    N_test_out[iii] = np.abs(N_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA on features from training set and also transform features from both test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "PCA_model = PCA(n_components=10)\n",
    "X_train_PCA = PCA_model.fit_transform(X_train_scaled)\n",
    "\n",
    "X_test_in_scaled = X_scaler.transform(X_test_in)\n",
    "X_test_in_PCA = PCA_model.transform(X_test_in_scaled)\n",
    "\n",
    "X_test_out_scaled = X_scaler.transform(X_test_out)\n",
    "X_test_out_PCA = PCA_model.transform(X_test_out_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all variables into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed PCA features\n",
    "np.savetxt('Processed input output/X_train_PCA.csv',X_train_PCA,delimiter=\",\")\n",
    "np.savetxt('Processed input output/X_test_in_PCA.csv',X_test_in_PCA,delimiter=\",\")\n",
    "np.savetxt('Processed input output/X_test_out_PCA.csv',X_test_out_PCA,delimiter=\",\")\n",
    "\n",
    "# Processed label for training\n",
    "np.savetxt('Processed input output/b_train.csv',b_train,delimiter=\",\")\n",
    "np.savetxt('Processed input output/knot_train.csv',knot_train,delimiter=\",\")\n",
    "\n",
    "# Processed ah-throughput at equidistant SOH\n",
    "np.savetxt('Processed input output/N_train.csv',N_train,delimiter=\",\")\n",
    "np.savetxt('Processed input output/N_test_in.csv',N_test_in,delimiter=\",\")\n",
    "np.savetxt('Processed input output/N_test_out.csv',N_test_out,delimiter=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
