{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient of variation, add warm-up for training the mean, implement $\\beta$-NLL loss function during training, 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error,root_mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import optuna\n",
    "import random\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cells = pd.read_csv(\"../Data_preprocessing/training.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "test_in_cells = pd.read_csv(\"../Data_preprocessing/test_in.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "test_out_cells = pd.read_csv(\"../Data_preprocessing/test_out.csv\",header=None).to_numpy(dtype=str).reshape(-1,).tolist()\n",
    "\n",
    "num_training_cells = len(training_cells)\n",
    "num_test_in_cells = len(test_in_cells)\n",
    "num_test_out_cells = len(test_out_cells)\n",
    "\n",
    "a = np.loadtxt('../Empirical_model_fitting/Empirical_parameters_global_train_py.csv').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_PCA = np.loadtxt(\"Processed_input_output/X_train_PCA.csv\",delimiter=\",\")\n",
    "X_test_in_PCA = np.loadtxt(\"Processed_input_output/X_test_in_PCA.csv\",delimiter=\",\")\n",
    "X_test_out_PCA = np.loadtxt(\"Processed_input_output/X_test_out_PCA.csv\",delimiter=\",\")\n",
    "\n",
    "N_train = np.loadtxt(\"Processed_input_output/N_train.csv\",delimiter=\",\")\n",
    "N_test_in = np.loadtxt(\"Processed_input_output/N_test_in.csv\",delimiter=\",\")\n",
    "N_test_out = np.loadtxt(\"Processed_input_output/N_test_out.csv\",delimiter=\",\")\n",
    "\n",
    "Q_train = np.array([np.linspace(1,0.8,21)] * num_training_cells)\n",
    "Q_test_in = np.array([np.linspace(1,0.8,21)] * num_test_in_cells)\n",
    "Q_test_out = np.array([np.linspace(1,0.8,21)] * num_test_out_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load predefined 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_cells_CV.pkl','rb') as f:\n",
    "    training_cells_CV = pickle.load(f)\n",
    "\n",
    "with open('val_cells_CV.pkl','rb') as f:\n",
    "    val_cells_CV = pickle.load(f)\n",
    "\n",
    "train_dataset_CV = []\n",
    "val_dataset_CV = []\n",
    "for fold in range(10):\n",
    "    train_cells = training_cells_CV[fold]\n",
    "    val_cells = val_cells_CV[fold]\n",
    "\n",
    "    X_train_fold = X_train_PCA[train_cells]\n",
    "    X_val_fold = X_train_PCA[val_cells]\n",
    "   \n",
    "    Y_train_fold = N_train[train_cells]\n",
    "    Y_val_fold = N_train[val_cells]\n",
    "\n",
    "    X_train_fold = torch.tensor(X_train_fold,dtype=torch.float32)\n",
    "    X_val_fold = torch.tensor(X_val_fold,dtype=torch.float32)\n",
    "\n",
    "    Y_train_fold = torch.tensor(Y_train_fold,dtype=torch.float32)\n",
    "    Y_val_fold = torch.tensor(Y_val_fold,dtype=torch.float32)\n",
    "\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_fold,Y_train_fold)\n",
    "    val_dataset = TensorDataset(X_val_fold,Y_val_fold)\n",
    "\n",
    "    train_dataset_CV.append(train_dataset)\n",
    "    val_dataset_CV.append(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define end-to-end objective function and empirical models for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_model(global_p,b1,b2,b3,N_eq,b_weight=[1e-3,1e2,1e2]):\n",
    "    a1 = global_p\n",
    "    # Match the scaling for end-to-end formulation\n",
    "    return 1 - b1*N_eq**a1*b_weight[0] - 1/(1+np.exp((b2*b_weight[1]-N_eq)/(b3*b_weight[2])))\n",
    "\n",
    "\n",
    "def empirical_model_seperate(global_p,b1,b2,b3,N_eq,b_weight=[1e-3,1e2,1e2]):\n",
    "    a1 = global_p\n",
    "    # Match the scaling for end-to-end formulation\n",
    "    return [1 - b1*N_eq**a1*b_weight[0] - 1/(1+np.exp((b2*b_weight[1]-N_eq)/(b3*b_weight[2]))),\n",
    "            - b1*N_eq**a1*b_weight[0],\n",
    "            - 1/(1+np.exp((b2*b_weight[1]-N_eq)/(b3*b_weight[2])))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 1: Predict the coefficient of variation of the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NN model function\n",
    "class Network_cof_variation(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(Network_cof_variation, self).__init__()\n",
    "        # Define the layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))  # Input layer\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))  # Hidden layers\n",
    "        self.mean_layer = nn.Linear(hidden_sizes[-1], output_size - 1)  # Output for b1, b2, b3\n",
    "        self.coe_var_layer = nn.Linear(hidden_sizes[-1], 1)  # Output for coefficient of variance\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through hidden layers with ReLU activations\n",
    "        for layer in self.layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        \n",
    "        # Separate outputs for means (b1, b2, b3) and coefficient of variance\n",
    "        # means = torch.sigmoid(self.mean_layer(x)) * (40 - 1) + 1  # Outputs in range [1, 40]\n",
    "        means = self.mean_layer(x)\n",
    "        # means = torch.relu(means)\n",
    "        means = torch.sigmoid(means) * (40 - 1) + 1  # Outputs in range [1, 30]\n",
    "\n",
    "        coe_var = F.softplus(self.coe_var_layer(x))  # Scale to [0, inf] for CV\n",
    "\n",
    "        return torch.cat((means, coe_var), dim=1)  # Concatenate means and coe_var\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # He Normal initialization for weights, zero for biases\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "                if layer.bias is not None:\n",
    "                    torch.nn.init.zeros_(layer.bias)\n",
    "                    \n",
    "        # Xavier Normal initialization for mean_layer (sigmoid activation)\n",
    "        torch.nn.init.xavier_normal_(self.mean_layer.weight)\n",
    "        # torch.nn.init.kaiming_normal_(self.mean_layer.weight, nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(self.mean_layer.bias)\n",
    "        \n",
    "        # Xavier Normal initialization for coe_var_layer (Softplus activation)\n",
    "        # torch.nn.init.xavier_normal_(self.coe_var_layer.weight)\n",
    "        torch.nn.init.ones_(self.coe_var_layer.bias)\n",
    "        torch.nn.init.zeros_(self.coe_var_layer.bias)\n",
    "\n",
    "\n",
    "# Define the early stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss functions\n",
    "# regular NLL loss for tracking traning performance\n",
    "# Beta-NLL loss for backpropagation\n",
    "\n",
    "def end_to_end_loss_NN_NLL_cov(output, target,a=0.5,warmup=True,b_weight=[1e-3,1e2,1e2]):\n",
    "    # batch size and number of measurements\n",
    "    (batch_size, len_interp) = target.size()\n",
    "\n",
    "    # Predicted means (b1, b2, b3) and coefficient of variation\n",
    "    b1 = output[:, 0].unsqueeze(1)  # b1 mean prediction\n",
    "    b2 = output[:, 1].unsqueeze(1)  # b2 mean prediction\n",
    "    b3 = output[:, 2].unsqueeze(1)  # b3 mean prediction\n",
    "    # log_coe_var = output[:, 3].unsqueeze(1)  # log of coefficient of variation for all predictions\n",
    "    coe_var = output[:, 3].unsqueeze(1)  # coefficient of variation for all predictions\n",
    "    \n",
    "    # Define ones\n",
    "    I_nm = torch.ones_like(target, dtype=torch.float32)\n",
    "    I_1m = torch.ones((1, len_interp), dtype=torch.float32)\n",
    "    \n",
    "    # Define Q (true capacity trajectories)\n",
    "    Q = torch.linspace(1, 0.8, len_interp).unsqueeze(0).repeat(batch_size, 1) * 100 # in percentage scale\n",
    "    \n",
    "    # Calculate term_2\n",
    "    term_2 = b1 * I_1m * b_weight[0] * torch.pow(target, a)\n",
    "    \n",
    "    # Calculate term_3\n",
    "    exp_input = ((b2 * I_1m * b_weight[1]) - target) / ((b3 * I_1m * b_weight[2]) + 1e-6)\n",
    "    # exp_input_clamped = 80 * torch.tanh(exp_input / 80)  # Smooth clipping using tanh to avoid overflow/underflow\n",
    "    exp_input_clamped = torch.clamp(exp_input, min=-80, max=80)  # Clamp to avoid overflow/underflow\n",
    "    term_3 = I_nm / (I_nm + torch.exp(exp_input_clamped))\n",
    "    \n",
    "    # Calculate predicted values based on term_2 and term_3\n",
    "    predicted_values = (I_nm  -  term_2 - term_3) * 100\n",
    "    # Calculate the variance and log-variance\n",
    "    std = (100 - predicted_values) * coe_var # standard deviation based on coefficient of variation. As capacity fade increases, variance increases\n",
    "    var = torch.pow(std, 2)\n",
    "    var = torch.clamp(var, min=1e-6)  # Clamp variance to avoid division by zero and log of zero\n",
    "    log_var = torch.log(var) \n",
    "       \n",
    "    if warmup:\n",
    "        log_var = 0\n",
    "        var = 1\n",
    "\n",
    "    # Negative log-likelihood loss based on Gaussian distribution        \n",
    "    nll_loss = 0.5 * torch.mean((Q - predicted_values) ** 2 / var + log_var)\n",
    "    \n",
    "    return nll_loss\n",
    "\n",
    "\n",
    "def end_to_end_loss_NN_betaNLL(output, target,beta=0.5,a=0.5,warmup=True,b_weight=[1e-3,1e2,1e2]):\n",
    "    # batch size and number of measurements\n",
    "    (batch_size, len_interp) = target.size()\n",
    "\n",
    "    # Predicted means (b1, b2, b3) and coefficient of variation\n",
    "    b1 = output[:, 0].unsqueeze(1)  # b1 mean prediction\n",
    "    b2 = output[:, 1].unsqueeze(1)  # b2 mean prediction\n",
    "    b3 = output[:, 2].unsqueeze(1)  # b3 mean prediction\n",
    "    # log_coe_var = output[:, 3].unsqueeze(1)  # log of coefficient of variation for all predictions\n",
    "    coe_var = output[:, 3].unsqueeze(1)  # coefficient of variation for all predictions\n",
    "    \n",
    "    # Define ones\n",
    "    I_nm = torch.ones_like(target, dtype=torch.float32)\n",
    "    I_1m = torch.ones((1, len_interp), dtype=torch.float32)\n",
    "    \n",
    "    # Define Q (true capacity trajectories)\n",
    "    Q = torch.linspace(1, 0.8, len_interp).unsqueeze(0).repeat(batch_size, 1) * 100 # in percentage scale\n",
    "    \n",
    "    # Calculate term_2\n",
    "    term_2 = b1 * I_1m * b_weight[0] * torch.pow(target, a)\n",
    "    \n",
    "    # Calculate term_3\n",
    "    exp_input = ((b2 * I_1m * b_weight[1]) - target) / ((b3 * I_1m * b_weight[2]) + 1e-6)\n",
    "    # exp_input_clamped = 80 * torch.tanh(exp_input / 80)  # Smooth clipping using tanh to avoid overflow/underflow\n",
    "    exp_input_clamped = torch.clamp(exp_input, min=-80, max=80)  # Clamp to avoid overflow/underflow\n",
    "    term_3 = I_nm / (I_nm + torch.exp(exp_input_clamped))\n",
    "    \n",
    "    # Calculate predicted values based on term_2 and term_3\n",
    "    predicted_values = (I_nm  -  term_2 - term_3) * 100\n",
    "    # Calculate the variance and log-variance\n",
    "    std = (100 - predicted_values) * coe_var # standard deviation based on coefficient of variation. As capacity fade increases, variance increases\n",
    "    var = torch.pow(std, 2)\n",
    "    var = torch.clamp(var, min=1e-6)  # Clamp variance to avoid division by zero and log of zero\n",
    "    log_var = torch.log(var) \n",
    "       \n",
    "    if warmup or beta==0:\n",
    "        log_var = 0\n",
    "        var = 1\n",
    "        # Negative log-likelihood loss based on Gaussian distribution        \n",
    "        nll_loss = 0.5 * torch.mean((Q - predicted_values) ** 2 / var + log_var)\n",
    "    else:\n",
    "        nll_loss = torch.mean((0.5 * ((Q - predicted_values) ** 2 / var + log_var)) * var.detach() ** beta)\n",
    "    \n",
    "    return nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization with 10-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up neural network functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ = 42\n",
    "random.seed(seed_)\n",
    "torch.manual_seed(seed_)\n",
    "np.random.seed(seed_)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data for training NN (create a validation subset for early stopping and model selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:33:38,757] A new study created in memory with name: no-name-1454c813-df69-420a-9050-b5f43ff5f674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 662\n",
      "Early stopping at epoch 665\n",
      "Early stopping at epoch 663\n",
      "Early stopping at epoch 658\n",
      "Early stopping at epoch 673\n",
      "Early stopping at epoch 671\n",
      "Early stopping at epoch 658\n",
      "Early stopping at epoch 660\n",
      "Early stopping at epoch 658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:33:56,861] Trial 0 finished with value: 5.872484445571899 and parameters: {'num_layer': 3, 'num_neuron': 10, 'batch_size': 42, 'lr': 0.007875660249889864, 'weight_decay': 2.0513382630874456e-08, 'warmup_epochs': 546}. Best is trial 0 with value: 5.872484445571899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 664\n",
      "Validation loss: [4.984686374664307, 18.311635971069336, 2.1316568851470947, 5.804723262786865, 3.1161296367645264, 3.59780216217041, 11.753974914550781, 3.308239221572876, 2.4160423278808594, 3.2999536991119385]\n",
      "Early stopping at epoch 924\n",
      "Early stopping at epoch 920\n",
      "Early stopping at epoch 905\n",
      "Early stopping at epoch 910\n",
      "Early stopping at epoch 917\n",
      "Early stopping at epoch 912\n",
      "Early stopping at epoch 903\n",
      "Early stopping at epoch 911\n",
      "Early stopping at epoch 915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:34:23,392] Trial 1 finished with value: 626095.0360554695 and parameters: {'num_layer': 2, 'num_neuron': 10, 'batch_size': 40, 'lr': 0.013035123791853833, 'weight_decay': 1.0994335574766189e-08, 'warmup_epochs': 791}. Best is trial 0 with value: 5.872484445571899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 903\n",
      "Validation loss: [4.04402494430542, 6260892.0, 3.4432997703552246, 5.707590579986572, 3.6203439235687256, 4.508126258850098, 2.239419460296631, 2.9309871196746826, 2.5648903846740723, 29.30187225341797]\n",
      "Early stopping at epoch 769\n",
      "Early stopping at epoch 769\n",
      "Early stopping at epoch 769\n",
      "Early stopping at epoch 776\n",
      "Early stopping at epoch 785\n",
      "Early stopping at epoch 769\n",
      "Early stopping at epoch 769\n",
      "Early stopping at epoch 912\n",
      "Early stopping at epoch 769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:34:50,118] Trial 2 finished with value: 3.0734760761260986 and parameters: {'num_layer': 5, 'num_neuron': 6, 'batch_size': 34, 'lr': 0.0011635338541918904, 'weight_decay': 4.0596116104842925e-08, 'warmup_epochs': 657}. Best is trial 2 with value: 3.0734760761260986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 769\n",
      "Validation loss: [4.482012748718262, 11.650896072387695, 1.8591282367706299, 2.888173818588257, 1.8872982263565063, 2.1303932666778564, 2.1271414756774902, 1.012237310409546, 1.390867829322815, 1.3066117763519287]\n",
      "Early stopping at epoch 723\n",
      "Early stopping at epoch 722\n",
      "Early stopping at epoch 728\n",
      "Early stopping at epoch 722\n",
      "Early stopping at epoch 745\n",
      "Early stopping at epoch 723\n",
      "Early stopping at epoch 746\n",
      "Early stopping at epoch 793\n",
      "Early stopping at epoch 857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:35:13,392] Trial 3 finished with value: 21.18133510351181 and parameters: {'num_layer': 3, 'num_neuron': 6, 'batch_size': 40, 'lr': 0.0009505122659935177, 'weight_decay': 3.8396292998041626e-08, 'warmup_epochs': 610}. Best is trial 2 with value: 3.0734760761260986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 831\n",
      "Validation loss: [3.449284791946411, 193.52212524414062, 1.6843541860580444, 4.559512615203857, 1.7659484148025513, 1.861024260520935, 1.2229540348052979, 1.167273998260498, 1.27794349193573, 1.3029299974441528]\n",
      "Early stopping at epoch 633\n",
      "Early stopping at epoch 626\n",
      "Early stopping at epoch 632\n",
      "Early stopping at epoch 625\n",
      "Early stopping at epoch 629\n",
      "Early stopping at epoch 633\n",
      "Early stopping at epoch 679\n",
      "Early stopping at epoch 630\n",
      "Early stopping at epoch 628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:35:33,140] Trial 4 finished with value: 14.381719279289246 and parameters: {'num_layer': 3, 'num_neuron': 9, 'batch_size': 34, 'lr': 0.005338741354740679, 'weight_decay': 1.530485212183145e-07, 'warmup_epochs': 513}. Best is trial 2 with value: 3.0734760761260986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 625\n",
      "Validation loss: [4.344485282897949, 1.9656119346618652, 11.739871978759766, 2.914477586746216, 2.0960886478424072, 4.276647090911865, 5.419413089752197, 101.91710662841797, 3.3872830867767334, 5.756207466125488]\n",
      "Early stopping at epoch 855\n",
      "Early stopping at epoch 858\n",
      "Early stopping at epoch 865\n",
      "Early stopping at epoch 856\n",
      "Early stopping at epoch 870\n",
      "Early stopping at epoch 858\n",
      "Early stopping at epoch 859\n",
      "Early stopping at epoch 855\n",
      "Early stopping at epoch 869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:36:00,340] Trial 5 finished with value: 4.763686883449554 and parameters: {'num_layer': 4, 'num_neuron': 6, 'batch_size': 32, 'lr': 0.039513097748541164, 'weight_decay': 8.536189862866822e-07, 'warmup_epochs': 743}. Best is trial 2 with value: 3.0734760761260986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 887\n",
      "Validation loss: [3.5443239212036133, 2.1599130630493164, 1.81303870677948, 2.7499871253967285, 2.411235809326172, 10.310951232910156, 7.573029041290283, 2.3210840225219727, 13.72806453704834, 1.0252413749694824]\n",
      "Early stopping at epoch 761\n",
      "Early stopping at epoch 794\n",
      "Early stopping at epoch 852\n",
      "Early stopping at epoch 776\n",
      "Early stopping at epoch 781\n",
      "Early stopping at epoch 761\n",
      "Early stopping at epoch 761\n",
      "Early stopping at epoch 761\n",
      "Early stopping at epoch 761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:36:20,462] Trial 6 finished with value: 1233.0753793239594 and parameters: {'num_layer': 3, 'num_neuron': 5, 'batch_size': 41, 'lr': 0.0037955524026413476, 'weight_decay': 1.754189348745074e-08, 'warmup_epochs': 649}. Best is trial 2 with value: 3.0734760761260986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 761\n",
      "Validation loss: [3.141395092010498, 12308.3486328125, 2.9675862789154053, 3.61737060546875, 1.6235605478286743, 3.2208151817321777, 1.3494588136672974, 1.4900447130203247, 3.3275880813598633, 1.6673411130905151]\n",
      "Early stopping at epoch 774\n",
      "Early stopping at epoch 786\n",
      "Early stopping at epoch 768\n",
      "Early stopping at epoch 776\n",
      "Early stopping at epoch 771\n",
      "Early stopping at epoch 768\n",
      "Early stopping at epoch 774\n",
      "Early stopping at epoch 787\n",
      "Early stopping at epoch 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:36:41,912] Trial 7 finished with value: 6.533229279518127 and parameters: {'num_layer': 2, 'num_neuron': 10, 'batch_size': 35, 'lr': 0.010568529720322864, 'weight_decay': 4.20167205437253e-08, 'warmup_epochs': 656}. Best is trial 2 with value: 3.0734760761260986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 769\n",
      "Validation loss: [4.361969470977783, 3.330785036087036, 6.816019535064697, 3.496380567550659, 6.047537803649902, 6.06876277923584, 9.39062786102295, 16.993896484375, 3.173154592514038, 5.653158664703369]\n",
      "Early stopping at epoch 881\n",
      "Early stopping at epoch 915\n",
      "Early stopping at epoch 893\n",
      "Early stopping at epoch 882\n",
      "Early stopping at epoch 890\n",
      "Early stopping at epoch 881\n",
      "Early stopping at epoch 882\n",
      "Early stopping at epoch 888\n",
      "Early stopping at epoch 886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:37:03,065] Trial 8 finished with value: 6.0268386721611025 and parameters: {'num_layer': 4, 'num_neuron': 6, 'batch_size': 45, 'lr': 0.017751524290641543, 'weight_decay': 7.568292060167619e-07, 'warmup_epochs': 769}. Best is trial 2 with value: 3.0734760761260986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 882\n",
      "Validation loss: [4.659225940704346, 21.550491333007812, 2.0994129180908203, 2.98039174079895, 2.540842056274414, 5.710744380950928, 8.207027435302734, 4.374622344970703, 6.398103713989258, 1.7475248575210571]\n",
      "Early stopping at epoch 720\n",
      "Early stopping at epoch 812\n",
      "Early stopping at epoch 731\n",
      "Early stopping at epoch 709\n",
      "Early stopping at epoch 714\n",
      "Early stopping at epoch 709\n",
      "Early stopping at epoch 709\n",
      "Early stopping at epoch 737\n",
      "Early stopping at epoch 709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:37:27,239] Trial 9 finished with value: 6.324145567417145 and parameters: {'num_layer': 4, 'num_neuron': 10, 'batch_size': 33, 'lr': 0.0012329223607243688, 'weight_decay': 1.2315571723665983e-08, 'warmup_epochs': 597}. Best is trial 2 with value: 3.0734760761260986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 788\n",
      "Validation loss: [1.7328518629074097, 45.40230178833008, 2.3411684036254883, 2.4623358249664307, 2.254539966583252, 2.5044820308685303, 2.1788101196289062, 1.1605217456817627, 1.470639705657959, 1.7338042259216309]\n",
      "Early stopping at epoch 819\n",
      "Early stopping at epoch 819\n",
      "Early stopping at epoch 976\n",
      "Early stopping at epoch 822\n",
      "Early stopping at epoch 888\n",
      "Early stopping at epoch 1090\n",
      "Early stopping at epoch 819\n",
      "Early stopping at epoch 921\n",
      "Early stopping at epoch 937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:37:58,899] Trial 10 finished with value: 3.8879655241966247 and parameters: {'num_layer': 5, 'num_neuron': 8, 'batch_size': 37, 'lr': 0.000543117448696857, 'weight_decay': 1.62833383399089e-07, 'warmup_epochs': 707}. Best is trial 2 with value: 3.0734760761260986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 910\n",
      "Validation loss: [7.365237712860107, 14.339088439941406, 1.0118458271026611, 4.648403167724609, 1.5466337203979492, 3.6550798416137695, 2.2684173583984375, 1.314466953277588, 1.307550311088562, 1.4229319095611572]\n",
      "Early stopping at epoch 821\n",
      "Early stopping at epoch 821\n",
      "Early stopping at epoch 821\n",
      "Early stopping at epoch 821\n",
      "Early stopping at epoch 821\n",
      "Early stopping at epoch 821\n",
      "Early stopping at epoch 821\n",
      "Early stopping at epoch 821\n",
      "Early stopping at epoch 1372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:38:29,011] Trial 11 finished with value: 2.9594900369644166 and parameters: {'num_layer': 5, 'num_neuron': 8, 'batch_size': 37, 'lr': 0.0005087534804133938, 'weight_decay': 1.9551476095874134e-07, 'warmup_epochs': 709}. Best is trial 11 with value: 2.9594900369644166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 821\n",
      "Validation loss: [2.8412606716156006, 2.210505485534668, 1.6052097082138062, 11.793872833251953, 1.9536724090576172, 2.5214591026306152, 1.6832605600357056, 1.325116515159607, 1.376670241355896, 2.2838728427886963]\n",
      "Early stopping at epoch 814\n",
      "Early stopping at epoch 834\n",
      "Early stopping at epoch 811\n",
      "Early stopping at epoch 811\n",
      "Early stopping at epoch 813\n",
      "Early stopping at epoch 819\n",
      "Early stopping at epoch 868\n",
      "Early stopping at epoch 811\n",
      "Early stopping at epoch 811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:38:56,831] Trial 12 finished with value: 43.190457582473755 and parameters: {'num_layer': 5, 'num_neuron': 8, 'batch_size': 36, 'lr': 0.0021654217996898086, 'weight_decay': 3.35188262509159e-07, 'warmup_epochs': 699}. Best is trial 11 with value: 2.9594900369644166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 812\n",
      "Validation loss: [8.125845909118652, 391.74163818359375, 2.9928529262542725, 3.907101631164551, 2.923259973526001, 3.056791067123413, 6.748580455780029, 3.639211893081665, 3.4226512908935547, 5.34664249420166]\n",
      "Early stopping at epoch 808\n",
      "Early stopping at epoch 808\n",
      "Early stopping at epoch 937\n",
      "Early stopping at epoch 808\n",
      "Early stopping at epoch 833\n",
      "Early stopping at epoch 808\n",
      "Early stopping at epoch 1085\n",
      "Early stopping at epoch 1348\n",
      "Early stopping at epoch 821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:39:28,061] Trial 13 finished with value: 1.8740141153335572 and parameters: {'num_layer': 5, 'num_neuron': 7, 'batch_size': 38, 'lr': 0.0005157728376864142, 'weight_decay': 7.116477814727243e-08, 'warmup_epochs': 696}. Best is trial 13 with value: 1.8740141153335572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 808\n",
      "Validation loss: [2.7062628269195557, 3.8604869842529297, 1.1323682069778442, 1.5499956607818604, 1.7582646608352661, 2.336493730545044, 1.393746018409729, 0.9106433391571045, 1.4410609006881714, 1.6508188247680664]\n",
      "Early stopping at epoch 825\n",
      "Early stopping at epoch 825\n",
      "Early stopping at epoch 860\n",
      "Early stopping at epoch 825\n",
      "Early stopping at epoch 825\n",
      "Early stopping at epoch 825\n",
      "Early stopping at epoch 825\n",
      "Early stopping at epoch 1126\n",
      "Early stopping at epoch 976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:39:59,069] Trial 14 finished with value: 2.8321723461151125 and parameters: {'num_layer': 5, 'num_neuron': 7, 'batch_size': 38, 'lr': 0.0005440211675273811, 'weight_decay': 1.0676916501410118e-07, 'warmup_epochs': 713}. Best is trial 13 with value: 1.8740141153335572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 1047\n",
      "Validation loss: [3.7359838485717773, 7.838833808898926, 1.37714684009552, 2.6634268760681152, 1.719116449356079, 2.057387590408325, 5.417938709259033, 1.373600959777832, 1.1275697946548462, 1.010718584060669]\n",
      "Early stopping at epoch 889\n",
      "Early stopping at epoch 1050\n",
      "Early stopping at epoch 861\n",
      "Early stopping at epoch 861\n",
      "Early stopping at epoch 875\n",
      "Early stopping at epoch 861\n",
      "Early stopping at epoch 927\n",
      "Early stopping at epoch 868\n",
      "Early stopping at epoch 924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:40:29,658] Trial 15 finished with value: 11.365179789066314 and parameters: {'num_layer': 5, 'num_neuron': 7, 'batch_size': 38, 'lr': 0.0022915500138460387, 'weight_decay': 7.793575055428743e-08, 'warmup_epochs': 749}. Best is trial 13 with value: 1.8740141153335572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 876\n",
      "Validation loss: [8.094882011413574, 48.8746223449707, 5.24083948135376, 6.351700305938721, 3.8377206325531006, 9.051167488098145, 24.875873565673828, 2.4178130626678467, 3.786302328109741, 1.120876669883728]\n",
      "Early stopping at epoch 798\n",
      "Early stopping at epoch 928\n",
      "Early stopping at epoch 798\n",
      "Early stopping at epoch 798\n",
      "Early stopping at epoch 798\n",
      "Early stopping at epoch 798\n",
      "Early stopping at epoch 1232\n",
      "Early stopping at epoch 878\n",
      "Early stopping at epoch 818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:40:50,383] Trial 16 finished with value: 1.8329599499702454 and parameters: {'num_layer': 4, 'num_neuron': 7, 'batch_size': 43, 'lr': 0.0007921636286268318, 'weight_decay': 7.827545905064409e-08, 'warmup_epochs': 686}. Best is trial 16 with value: 1.8329599499702454.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 1012\n",
      "Validation loss: [2.4322516918182373, 1.5127856731414795, 3.1769630908966064, 2.2424676418304443, 1.7485451698303223, 1.9528415203094482, 1.0844274759292603, 1.4327366352081299, 1.419731855392456, 1.3268487453460693]\n",
      "Early stopping at epoch 732\n",
      "Early stopping at epoch 851\n",
      "Early stopping at epoch 732\n",
      "Early stopping at epoch 746\n",
      "Early stopping at epoch 732\n",
      "Early stopping at epoch 753\n",
      "Early stopping at epoch 732\n",
      "Early stopping at epoch 732\n",
      "Early stopping at epoch 792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:41:07,962] Trial 17 finished with value: 3.336972141265869 and parameters: {'num_layer': 4, 'num_neuron': 7, 'batch_size': 44, 'lr': 0.001857439054130012, 'weight_decay': 8.588420808476909e-08, 'warmup_epochs': 620}. Best is trial 16 with value: 1.8329599499702454.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 732\n",
      "Validation loss: [2.525721549987793, 2.1658737659454346, 1.4292676448822021, 6.396155834197998, 1.4877899885177612, 6.811934947967529, 6.6810150146484375, 2.9739274978637695, 1.2596304416656494, 1.6384047269821167]\n",
      "Early stopping at epoch 792\n",
      "Early stopping at epoch 792\n",
      "Early stopping at epoch 814\n",
      "Early stopping at epoch 858\n",
      "Early stopping at epoch 818\n",
      "Early stopping at epoch 803\n",
      "Early stopping at epoch 1039\n",
      "Early stopping at epoch 967\n",
      "Early stopping at epoch 792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:41:31,538] Trial 18 finished with value: 1.690043318271637 and parameters: {'num_layer': 4, 'num_neuron': 5, 'batch_size': 42, 'lr': 0.0008863564337367059, 'weight_decay': 3.327528914795916e-07, 'warmup_epochs': 680}. Best is trial 18 with value: 1.690043318271637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 1024\n",
      "Validation loss: [2.604918956756592, 2.1824564933776855, 1.9211866855621338, 1.6293836832046509, 1.4592219591140747, 1.6823105812072754, 1.0117651224136353, 1.3369876146316528, 1.6722416877746582, 1.3999603986740112]\n",
      "Early stopping at epoch 691\n",
      "Early stopping at epoch 718\n",
      "Early stopping at epoch 801\n",
      "Early stopping at epoch 686\n",
      "Early stopping at epoch 688\n",
      "Early stopping at epoch 678\n",
      "Early stopping at epoch 706\n",
      "Early stopping at epoch 696\n",
      "Early stopping at epoch 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:41:48,253] Trial 19 finished with value: 23.73389815092087 and parameters: {'num_layer': 4, 'num_neuron': 5, 'batch_size': 43, 'lr': 0.003714559377762824, 'weight_decay': 3.8409532513874557e-07, 'warmup_epochs': 564}. Best is trial 18 with value: 1.690043318271637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 676\n",
      "Validation loss: [24.030811309814453, 2.2429676055908203, 2.4487504959106445, 2.3902032375335693, 3.04439640045166, 7.521435737609863, 1.1211196184158325, 191.0987548828125, 1.529484510421753, 1.911057710647583]\n",
      "Early stopping at epoch 790\n",
      "Early stopping at epoch 1322\n",
      "Early stopping at epoch 858\n",
      "Early stopping at epoch 779\n",
      "Early stopping at epoch 806\n",
      "Early stopping at epoch 795\n",
      "Early stopping at epoch 779\n",
      "Early stopping at epoch 969\n",
      "Early stopping at epoch 1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:42:09,800] Trial 20 finished with value: 1.6530466556549073 and parameters: {'num_layer': 4, 'num_neuron': 5, 'batch_size': 43, 'lr': 0.0010279980160927279, 'weight_decay': 3.5722029462803684e-07, 'warmup_epochs': 667}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 881\n",
      "Validation loss: [2.363574266433716, 2.3475871086120605, 0.9635247588157654, 2.4361093044281006, 1.4927858114242554, 1.767439365386963, 1.592334270477295, 0.9360367655754089, 1.2372785806655884, 1.3937963247299194]\n",
      "Early stopping at epoch 787\n",
      "Early stopping at epoch 787\n",
      "Early stopping at epoch 787\n",
      "Early stopping at epoch 787\n",
      "Early stopping at epoch 787\n",
      "Early stopping at epoch 787\n",
      "Early stopping at epoch 988\n",
      "Early stopping at epoch 1063\n",
      "Early stopping at epoch 788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:42:29,964] Trial 21 finished with value: 23.491838896274565 and parameters: {'num_layer': 4, 'num_neuron': 5, 'batch_size': 43, 'lr': 0.0008754478152979281, 'weight_decay': 4.291466392054536e-07, 'warmup_epochs': 675}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 827\n",
      "Validation loss: [1.8298572301864624, 217.9237823486328, 1.5219717025756836, 3.1492886543273926, 1.3913012742996216, 1.730467677116394, 1.4557448625564575, 1.1109563112258911, 3.4149482250213623, 1.3900706768035889]\n",
      "Early stopping at epoch 754\n",
      "Early stopping at epoch 760\n",
      "Early stopping at epoch 777\n",
      "Early stopping at epoch 841\n",
      "Early stopping at epoch 737\n",
      "Early stopping at epoch 737\n",
      "Early stopping at epoch 737\n",
      "Early stopping at epoch 756\n",
      "Early stopping at epoch 795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:42:48,555] Trial 22 finished with value: 2.4700465559959413 and parameters: {'num_layer': 4, 'num_neuron': 5, 'batch_size': 45, 'lr': 0.0014423514273666199, 'weight_decay': 2.9945724537527633e-07, 'warmup_epochs': 625}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 929\n",
      "Validation loss: [2.447892189025879, 1.9935511350631714, 1.3866592645645142, 3.2277743816375732, 2.377533435821533, 1.8241283893585205, 3.8945224285125732, 1.4500231742858887, 4.898909091949463, 1.1994720697402954]\n",
      "Early stopping at epoch 871\n",
      "Early stopping at epoch 1240\n",
      "Early stopping at epoch 953\n",
      "Early stopping at epoch 849\n",
      "Early stopping at epoch 849\n",
      "Early stopping at epoch 867\n",
      "Early stopping at epoch 989\n",
      "Early stopping at epoch 934\n",
      "Early stopping at epoch 849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:43:12,735] Trial 23 finished with value: 2.08427038192749 and parameters: {'num_layer': 4, 'num_neuron': 6, 'batch_size': 42, 'lr': 0.0008316493537526588, 'weight_decay': 5.480752357740461e-07, 'warmup_epochs': 737}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 851\n",
      "Validation loss: [4.718855381011963, 1.578282356262207, 1.2047888040542603, 2.4718306064605713, 1.877047061920166, 2.6601035594940186, 1.4482961893081665, 1.3982435464859009, 1.8133643865585327, 1.6718919277191162]\n",
      "Early stopping at epoch 793\n",
      "Early stopping at epoch 793\n",
      "Early stopping at epoch 992\n",
      "Early stopping at epoch 796\n",
      "Early stopping at epoch 846\n",
      "Early stopping at epoch 793\n",
      "Early stopping at epoch 1025\n",
      "Early stopping at epoch 960\n",
      "Early stopping at epoch 971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:43:40,008] Trial 24 finished with value: 2.497421908378601 and parameters: {'num_layer': 3, 'num_neuron': 5, 'batch_size': 40, 'lr': 0.00076184745827792, 'weight_decay': 2.1865798444280878e-07, 'warmup_epochs': 681}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 994\n",
      "Validation loss: [3.2462503910064697, 7.37576150894165, 1.3313894271850586, 2.585503578186035, 1.5566712617874146, 1.9001423120498657, 1.3061555624008179, 1.3411153554916382, 3.312143564224243, 1.0190861225128174]\n",
      "Early stopping at epoch 754\n",
      "Early stopping at epoch 754\n",
      "Early stopping at epoch 754\n",
      "Early stopping at epoch 754\n",
      "Early stopping at epoch 765\n",
      "Early stopping at epoch 784\n",
      "Early stopping at epoch 754\n",
      "Early stopping at epoch 754\n",
      "Early stopping at epoch 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:43:58,049] Trial 25 finished with value: 83.04926553964614 and parameters: {'num_layer': 4, 'num_neuron': 9, 'batch_size': 43, 'lr': 0.0016601633605047973, 'weight_decay': 5.865015693022449e-07, 'warmup_epochs': 642}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 754\n",
      "Validation loss: [4.244340896606445, 791.4924926757812, 7.020713806152344, 8.366578102111816, 7.474327564239502, 2.1506175994873047, 2.125896692276001, 1.4459789991378784, 4.739085674285889, 1.4326233863830566]\n",
      "Early stopping at epoch 702\n",
      "Early stopping at epoch 702\n",
      "Early stopping at epoch 722\n",
      "Early stopping at epoch 704\n",
      "Early stopping at epoch 715\n",
      "Early stopping at epoch 706\n",
      "Early stopping at epoch 702\n",
      "Early stopping at epoch 1190\n",
      "Early stopping at epoch 708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:44:16,851] Trial 26 finished with value: 3.3126475334167482 and parameters: {'num_layer': 3, 'num_neuron': 5, 'batch_size': 41, 'lr': 0.0028975985879699205, 'weight_decay': 2.5615723280490915e-07, 'warmup_epochs': 590}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 702\n",
      "Validation loss: [3.0575380325317383, 9.19213581085205, 1.2863355875015259, 4.521904945373535, 1.6691129207611084, 1.810185432434082, 1.4256222248077393, 1.0740588903427124, 1.4398021697998047, 7.649779319763184]\n",
      "Early stopping at epoch 784\n",
      "Early stopping at epoch 784\n",
      "Early stopping at epoch 1012\n",
      "Early stopping at epoch 798\n",
      "Early stopping at epoch 904\n",
      "Early stopping at epoch 898\n",
      "Early stopping at epoch 1153\n",
      "Early stopping at epoch 907\n",
      "Early stopping at epoch 828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:44:38,049] Trial 27 finished with value: 334.4234673976898 and parameters: {'num_layer': 4, 'num_neuron': 6, 'batch_size': 44, 'lr': 0.000742961969614579, 'weight_decay': 1.2416455346521106e-07, 'warmup_epochs': 672}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 784\n",
      "Validation loss: [2.4703586101531982, 3325.38720703125, 1.146224021911621, 4.219516754150391, 1.3974502086639404, 4.2265472412109375, 1.2630754709243774, 1.3970719575881958, 1.1961982250213623, 1.53102445602417]\n",
      "Early stopping at epoch 840\n",
      "Early stopping at epoch 840\n",
      "Early stopping at epoch 840\n",
      "Early stopping at epoch 855\n",
      "Early stopping at epoch 900\n",
      "Early stopping at epoch 841\n",
      "Early stopping at epoch 842\n",
      "Early stopping at epoch 870\n",
      "Early stopping at epoch 840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:45:00,113] Trial 28 finished with value: 12.552148580551147 and parameters: {'num_layer': 4, 'num_neuron': 9, 'batch_size': 42, 'lr': 0.0012893820029605968, 'weight_decay': 5.5626311077153354e-08, 'warmup_epochs': 728}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 846\n",
      "Validation loss: [7.057521820068359, 83.93486785888672, 3.9204909801483154, 4.7977399826049805, 1.5536905527114868, 2.1778883934020996, 16.980709075927734, 1.537528157234192, 1.592538595199585, 1.968510389328003]\n",
      "Early stopping at epoch 760\n",
      "Early stopping at epoch 749\n",
      "Early stopping at epoch 749\n",
      "Early stopping at epoch 776\n",
      "Early stopping at epoch 750\n",
      "Early stopping at epoch 749\n",
      "Early stopping at epoch 749\n",
      "Early stopping at epoch 757\n",
      "Early stopping at epoch 749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:45:16,637] Trial 29 finished with value: 411.1581172823906 and parameters: {'num_layer': 3, 'num_neuron': 5, 'batch_size': 44, 'lr': 0.006898917564876366, 'weight_decay': 2.586200324205114e-08, 'warmup_epochs': 637}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 759\n",
      "Validation loss: [4.4124064445495605, 4081.241455078125, 3.153385877609253, 1.884336233139038, 2.4139723777770996, 2.420827627182007, 2.367650032043457, 2.2071540355682373, 1.1621407270431519, 10.31784439086914]\n",
      "Early stopping at epoch 685\n",
      "Early stopping at epoch 685\n",
      "Early stopping at epoch 703\n",
      "Early stopping at epoch 687\n",
      "Early stopping at epoch 685\n",
      "Early stopping at epoch 708\n",
      "Early stopping at epoch 685\n",
      "Early stopping at epoch 685\n",
      "Early stopping at epoch 767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:45:33,124] Trial 30 finished with value: 2.7874136567115784 and parameters: {'num_layer': 2, 'num_neuron': 6, 'batch_size': 41, 'lr': 0.0024754306443546172, 'weight_decay': 1.2668717196846585e-07, 'warmup_epochs': 573}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 685\n",
      "Validation loss: [2.7133119106292725, 5.859895706176758, 1.4895719289779663, 5.465113162994385, 3.009995937347412, 2.8637046813964844, 1.8994611501693726, 2.1997015476226807, 1.1056411266326904, 1.2677394151687622]\n",
      "Early stopping at epoch 796\n",
      "Early stopping at epoch 796\n",
      "Early stopping at epoch 897\n",
      "Early stopping at epoch 796\n",
      "Early stopping at epoch 828\n",
      "Early stopping at epoch 796\n",
      "Early stopping at epoch 1080\n",
      "Early stopping at epoch 887\n",
      "Early stopping at epoch 1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:46:02,855] Trial 31 finished with value: 13.179349398612976 and parameters: {'num_layer': 5, 'num_neuron': 7, 'batch_size': 39, 'lr': 0.0006685739657215396, 'weight_decay': 7.162755250169641e-08, 'warmup_epochs': 684}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 796\n",
      "Validation loss: [5.908875465393066, 113.97592163085938, 1.1439120769500732, 1.9256428480148315, 1.617347002029419, 1.6208544969558716, 1.2384445667266846, 1.5123567581176758, 1.2129441499710083, 1.6371949911117554]\n",
      "Early stopping at epoch 809\n",
      "Early stopping at epoch 809\n",
      "Early stopping at epoch 953\n",
      "Early stopping at epoch 809\n",
      "Early stopping at epoch 977\n",
      "Early stopping at epoch 819\n",
      "Early stopping at epoch 809\n",
      "Early stopping at epoch 809\n",
      "Early stopping at epoch 1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:46:32,421] Trial 32 finished with value: 1.829542601108551 and parameters: {'num_layer': 5, 'num_neuron': 7, 'batch_size': 39, 'lr': 0.0006475580747041781, 'weight_decay': 6.133187661413388e-08, 'warmup_epochs': 697}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 822\n",
      "Validation loss: [1.9533346891403198, 2.3698832988739014, 1.2519745826721191, 2.0545361042022705, 1.461111307144165, 2.2798423767089844, 2.1327426433563232, 1.592009425163269, 1.3528472185134888, 1.847144365310669]\n",
      "Early stopping at epoch 778\n",
      "Early stopping at epoch 778\n",
      "Early stopping at epoch 779\n",
      "Early stopping at epoch 778\n",
      "Early stopping at epoch 778\n",
      "Early stopping at epoch 778\n",
      "Early stopping at epoch 793\n",
      "Early stopping at epoch 778\n",
      "Early stopping at epoch 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:46:53,001] Trial 33 finished with value: 5.71186615228653 and parameters: {'num_layer': 4, 'num_neuron': 8, 'batch_size': 42, 'lr': 0.000999889965690374, 'weight_decay': 5.164245436141689e-08, 'warmup_epochs': 666}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 815\n",
      "Validation loss: [3.978311777114868, 3.316807270050049, 20.878786087036133, 10.10969066619873, 1.9269797801971436, 3.2032854557037354, 1.3987095355987549, 9.910924911499023, 1.051570177078247, 1.3435958623886108]\n",
      "Early stopping at epoch 910\n",
      "Early stopping at epoch 1028\n",
      "Early stopping at epoch 910\n",
      "Early stopping at epoch 1031\n",
      "Early stopping at epoch 933\n",
      "Early stopping at epoch 910\n",
      "Early stopping at epoch 910\n",
      "Early stopping at epoch 932\n",
      "Early stopping at epoch 910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:47:24,763] Trial 34 finished with value: 502.94523190259935 and parameters: {'num_layer': 5, 'num_neuron': 6, 'batch_size': 39, 'lr': 0.0011018640129482418, 'weight_decay': 2.8442616625166863e-08, 'warmup_epochs': 798}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 954\n",
      "Validation loss: [4.956273555755615, 5010.6806640625, 1.638257622718811, 1.8976649045944214, 1.5778446197509766, 2.421668529510498, 1.5752662420272827, 1.612991213798523, 1.8790549039840698, 1.2126333713531494]\n",
      "Early stopping at epoch 876\n",
      "Early stopping at epoch 936\n",
      "Early stopping at epoch 876\n",
      "Early stopping at epoch 879\n",
      "Early stopping at epoch 876\n",
      "Early stopping at epoch 876\n",
      "Early stopping at epoch 876\n",
      "Early stopping at epoch 876\n",
      "Early stopping at epoch 889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:47:52,723] Trial 35 finished with value: 5.5001971364021305 and parameters: {'num_layer': 4, 'num_neuron': 7, 'batch_size': 40, 'lr': 0.0014953879951575294, 'weight_decay': 5.7482144523503304e-08, 'warmup_epochs': 764}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 877\n",
      "Validation loss: [5.565990447998047, 34.62360382080078, 1.872551441192627, 2.2595372200012207, 1.7779483795166016, 3.2338593006134033, 1.535168170928955, 1.4777717590332031, 1.1472920179367065, 1.5082488059997559]\n",
      "Early stopping at epoch 835\n",
      "Early stopping at epoch 835\n",
      "Early stopping at epoch 835\n",
      "Early stopping at epoch 835\n",
      "Early stopping at epoch 971\n",
      "Early stopping at epoch 835\n",
      "Early stopping at epoch 835\n",
      "Early stopping at epoch 1358\n",
      "Early stopping at epoch 932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:48:17,739] Trial 36 finished with value: 2.995285177230835 and parameters: {'num_layer': 5, 'num_neuron': 6, 'batch_size': 42, 'lr': 0.0010376345888079717, 'weight_decay': 5.26670124411113e-07, 'warmup_epochs': 723}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 867\n",
      "Validation loss: [6.287963390350342, 2.80173397064209, 1.874274492263794, 3.3468875885009766, 1.2191922664642334, 2.2812817096710205, 1.3279927968978882, 8.06800651550293, 1.2359061241149902, 1.5096129179000854]\n",
      "Early stopping at epoch 804\n",
      "Early stopping at epoch 804\n",
      "Early stopping at epoch 804\n",
      "Early stopping at epoch 804\n",
      "Early stopping at epoch 973\n",
      "Early stopping at epoch 804\n",
      "Early stopping at epoch 981\n",
      "Early stopping at epoch 804\n",
      "Early stopping at epoch 1104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:48:37,957] Trial 37 finished with value: 33.3797945857048 and parameters: {'num_layer': 3, 'num_neuron': 5, 'batch_size': 43, 'lr': 0.0006829263203664288, 'weight_decay': 1.767595687343601e-07, 'warmup_epochs': 692}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 943\n",
      "Validation loss: [3.1616859436035156, 314.4872741699219, 1.5014514923095703, 2.0608761310577393, 1.5076338052749634, 5.139105796813965, 1.3289176225662231, 1.984518051147461, 1.3257877826690674, 1.3006950616836548]\n",
      "Early stopping at epoch 627\n",
      "Early stopping at epoch 627\n",
      "Early stopping at epoch 627\n",
      "Early stopping at epoch 627\n",
      "Early stopping at epoch 627\n",
      "Early stopping at epoch 627\n",
      "Early stopping at epoch 810\n",
      "Early stopping at epoch 627\n",
      "Early stopping at epoch 1248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:48:57,465] Trial 38 finished with value: 2.351237678527832 and parameters: {'num_layer': 4, 'num_neuron': 9, 'batch_size': 41, 'lr': 0.0006935390519732802, 'weight_decay': 3.185475576021876e-08, 'warmup_epochs': 515}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 629\n",
      "Validation loss: [2.7791337966918945, 4.684900760650635, 2.701572895050049, 3.089765787124634, 1.691279411315918, 1.6694422960281372, 1.466521143913269, 2.6966488361358643, 1.0199424028396606, 1.7131694555282593]\n",
      "Early stopping at epoch 782\n",
      "Early stopping at epoch 791\n",
      "Early stopping at epoch 780\n",
      "Early stopping at epoch 789\n",
      "Early stopping at epoch 783\n",
      "Early stopping at epoch 783\n",
      "Early stopping at epoch 785\n",
      "Early stopping at epoch 796\n",
      "Early stopping at epoch 782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:49:15,442] Trial 39 finished with value: 10.606049585342408 and parameters: {'num_layer': 3, 'num_neuron': 6, 'batch_size': 45, 'lr': 0.0176127921798861, 'weight_decay': 2.4602307418366513e-07, 'warmup_epochs': 667}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 864\n",
      "Validation loss: [3.647447347640991, 3.0135748386383057, 2.098081111907959, 2.330118179321289, 6.119406223297119, 60.887393951416016, 6.102048873901367, 3.5381040573120117, 6.229271411895752, 12.095049858093262]\n",
      "Early stopping at epoch 779\n",
      "Early stopping at epoch 768\n",
      "Early stopping at epoch 783\n",
      "Early stopping at epoch 771\n",
      "Early stopping at epoch 770\n",
      "Early stopping at epoch 769\n",
      "Early stopping at epoch 772\n",
      "Early stopping at epoch 768\n",
      "Early stopping at epoch 809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:49:31,955] Trial 40 finished with value: 11.869680345058441 and parameters: {'num_layer': 2, 'num_neuron': 7, 'batch_size': 44, 'lr': 0.03681805508420043, 'weight_decay': 9.958679628424277e-08, 'warmup_epochs': 654}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 767\n",
      "Validation loss: [3.039391279220581, 3.751966953277588, 1.9068611860275269, 5.090364933013916, 83.03450012207031, 6.426702499389648, 2.855694055557251, 3.0895538330078125, 3.43796968460083, 6.063798904418945]\n",
      "Early stopping at epoch 805\n",
      "Early stopping at epoch 849\n",
      "Early stopping at epoch 1026\n",
      "Early stopping at epoch 805\n",
      "Early stopping at epoch 827\n",
      "Early stopping at epoch 806\n",
      "Early stopping at epoch 1032\n",
      "Early stopping at epoch 805\n",
      "Early stopping at epoch 1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:50:04,415] Trial 41 finished with value: 2.0709542274475097 and parameters: {'num_layer': 5, 'num_neuron': 7, 'batch_size': 37, 'lr': 0.0006167569298124421, 'weight_decay': 6.630506658035215e-08, 'warmup_epochs': 693}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 1010\n",
      "Validation loss: [2.768022060394287, 1.7611624002456665, 1.1501795053482056, 2.012138843536377, 1.6140601634979248, 1.7912285327911377, 1.201965093612671, 5.597176551818848, 1.234701156616211, 1.5789079666137695]\n",
      "Early stopping at epoch 831\n",
      "Early stopping at epoch 831\n",
      "Early stopping at epoch 831\n",
      "Early stopping at epoch 831\n",
      "Early stopping at epoch 842\n",
      "Early stopping at epoch 831\n",
      "Early stopping at epoch 968\n",
      "Early stopping at epoch 838\n",
      "Early stopping at epoch 831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:50:33,789] Trial 42 finished with value: 2.32921359539032 and parameters: {'num_layer': 5, 'num_neuron': 8, 'batch_size': 36, 'lr': 0.000503977249453131, 'weight_decay': 4.330512763238845e-08, 'warmup_epochs': 719}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 1041\n",
      "Validation loss: [3.6145782470703125, 5.13544225692749, 2.0099306106567383, 2.4517629146575928, 1.7613015174865723, 2.9076576232910156, 1.244571328163147, 1.600304126739502, 1.5010364055633545, 1.0655509233474731]\n",
      "Early stopping at epoch 810\n",
      "Early stopping at epoch 954\n",
      "Early stopping at epoch 812\n",
      "Early stopping at epoch 810\n",
      "Early stopping at epoch 810\n",
      "Early stopping at epoch 810\n",
      "Early stopping at epoch 872\n",
      "Early stopping at epoch 822\n",
      "Early stopping at epoch 817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:51:02,261] Trial 43 finished with value: 2.17194344997406 and parameters: {'num_layer': 5, 'num_neuron': 7, 'batch_size': 39, 'lr': 0.000910213642410022, 'weight_decay': 1.2733860322964777e-07, 'warmup_epochs': 698}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 810\n",
      "Validation loss: [4.8741888999938965, 2.437579393386841, 1.6925013065338135, 3.1888020038604736, 1.7573367357254028, 1.7929259538650513, 1.4514870643615723, 1.437082052230835, 1.572710633277893, 1.5148204565048218]\n",
      "Early stopping at epoch 748\n",
      "Early stopping at epoch 747\n",
      "Early stopping at epoch 747\n",
      "Early stopping at epoch 779\n",
      "Early stopping at epoch 781\n",
      "Early stopping at epoch 747\n",
      "Early stopping at epoch 934\n",
      "Early stopping at epoch 747\n",
      "Early stopping at epoch 905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:51:29,460] Trial 44 finished with value: 1.7654189467430115 and parameters: {'num_layer': 5, 'num_neuron': 6, 'batch_size': 40, 'lr': 0.0011778765704150004, 'weight_decay': 9.29353320962666e-07, 'warmup_epochs': 635}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 747\n",
      "Validation loss: [2.328446865081787, 1.9397014379501343, 1.5939279794692993, 2.705760955810547, 1.5443949699401855, 1.9633712768554688, 1.4595600366592407, 1.3475892543792725, 1.0923302173614502, 1.6791064739227295]\n",
      "Early stopping at epoch 743\n",
      "Early stopping at epoch 918\n",
      "Early stopping at epoch 743\n",
      "Early stopping at epoch 743\n",
      "Early stopping at epoch 743\n",
      "Early stopping at epoch 748\n",
      "Early stopping at epoch 770\n",
      "Early stopping at epoch 752\n",
      "Early stopping at epoch 798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:51:55,071] Trial 45 finished with value: 1.973783028125763 and parameters: {'num_layer': 4, 'num_neuron': 6, 'batch_size': 40, 'lr': 0.0011535512844887542, 'weight_decay': 9.923983747934685e-07, 'warmup_epochs': 631}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 805\n",
      "Validation loss: [3.2976832389831543, 1.6099762916564941, 1.549251914024353, 3.616166830062866, 1.7024991512298584, 2.3257062435150146, 1.260523796081543, 1.4840245246887207, 1.4993805885314941, 1.3926177024841309]\n",
      "Early stopping at epoch 735\n",
      "Early stopping at epoch 724\n",
      "Early stopping at epoch 724\n",
      "Early stopping at epoch 724\n",
      "Early stopping at epoch 724\n",
      "Early stopping at epoch 731\n",
      "Early stopping at epoch 786\n",
      "Early stopping at epoch 724\n",
      "Early stopping at epoch 724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:52:16,779] Trial 46 finished with value: 2.6541247844696043 and parameters: {'num_layer': 5, 'num_neuron': 5, 'batch_size': 41, 'lr': 0.0017916033639855309, 'weight_decay': 7.069234553288865e-07, 'warmup_epochs': 612}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 828\n",
      "Validation loss: [4.411935806274414, 5.6447858810424805, 1.2157642841339111, 3.964144229888916, 2.290989398956299, 1.66295325756073, 1.3178616762161255, 1.5764994621276855, 1.700002908706665, 2.7563109397888184]\n",
      "Early stopping at epoch 765\n",
      "Early stopping at epoch 765\n",
      "Early stopping at epoch 765\n",
      "Early stopping at epoch 801\n",
      "Early stopping at epoch 765\n",
      "Early stopping at epoch 765\n",
      "Early stopping at epoch 783\n",
      "Early stopping at epoch 765\n",
      "Early stopping at epoch 765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:52:37,197] Trial 47 finished with value: 613896.6507045984 and parameters: {'num_layer': 4, 'num_neuron': 6, 'batch_size': 42, 'lr': 0.0013511201888522573, 'weight_decay': 4.515033487719981e-07, 'warmup_epochs': 653}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 765\n",
      "Validation loss: [2.55228328704834, 6138947.5, 2.5385477542877197, 1.903812050819397, 1.727954626083374, 3.548551559448242, 1.4167394638061523, 1.9315441846847534, 1.5205374956130981, 1.8670755624771118]\n",
      "Early stopping at epoch 771\n",
      "Early stopping at epoch 783\n",
      "Early stopping at epoch 771\n",
      "Early stopping at epoch 773\n",
      "Early stopping at epoch 771\n",
      "Early stopping at epoch 771\n",
      "Early stopping at epoch 771\n",
      "Early stopping at epoch 771\n",
      "Early stopping at epoch 775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:53:03,484] Trial 48 finished with value: 75912.64843539 and parameters: {'num_layer': 5, 'num_neuron': 5, 'batch_size': 40, 'lr': 0.004789444071084909, 'weight_decay': 7.282281486411192e-07, 'warmup_epochs': 659}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 777\n",
      "Validation loss: [3.019744634628296, 759094.5, 1.223127841949463, 3.4810173511505127, 2.444221258163452, 9.436936378479004, 7.002040386199951, 1.4709240198135376, 2.1856231689453125, 1.7207188606262207]\n",
      "Early stopping at epoch 705\n",
      "Early stopping at epoch 705\n",
      "Early stopping at epoch 705\n",
      "Early stopping at epoch 724\n",
      "Early stopping at epoch 1125\n",
      "Early stopping at epoch 705\n",
      "Early stopping at epoch 705\n",
      "Early stopping at epoch 927\n",
      "Early stopping at epoch 770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:53:22,869] Trial 49 finished with value: 1.774815607070923 and parameters: {'num_layer': 4, 'num_neuron': 6, 'batch_size': 43, 'lr': 0.0009027238512416587, 'weight_decay': 1.861176803145822e-08, 'warmup_epochs': 593}. Best is trial 20 with value: 1.6530466556549073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 790\n",
      "Validation loss: [2.1141748428344727, 2.304706573486328, 1.4957414865493774, 2.028783082962036, 1.3187745809555054, 2.9198496341705322, 1.237215280532837, 1.5420382022857666, 1.3971918821334839, 1.3896805047988892]\n",
      "Best hyperparameters: {'num_layer': 4, 'num_neuron': 5, 'batch_size': 43, 'lr': 0.0010279980160927279, 'weight_decay': 3.5722029462803684e-07, 'warmup_epochs': 667}\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = np.inf\n",
    "best_model_path = \"Best_network/best_NN_NLL_cov_V2.pth\"\n",
    "b_weight = [5e-4,30,5] # Scaling weights for b1, b2, b3\n",
    "def objective(trial):\n",
    "    global best_val_loss\n",
    "\n",
    "    val_loss_CV = []\n",
    "    model_state_dict = {}\n",
    "\n",
    "    # Define the input and output sizes\n",
    "    input_size = 10  # 10 PCA features\n",
    "    output_size = 4  # 3 empirical model parameters + 1 variance related value\n",
    "    beta = 0.5 # Beta value for Beta-NLL loss\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # Define the hyperparameters to tune\n",
    "    num_layer = trial.suggest_int('num_layer', 2, 5)\n",
    "    num_neuron = trial.suggest_int('num_neuron', 5, 10)\n",
    "    hidden_sizes = [num_neuron] * num_layer\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 45)\n",
    "    lr = trial.suggest_float('lr', 5e-4, 5e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-8, 1e-6, log=True)\n",
    "    warmup_epochs = trial.suggest_int('warmup_epochs', 500, 800)\n",
    "\n",
    "    for fold in range(10):\n",
    "        train_dataset = train_dataset_CV[fold]\n",
    "        val_dataset = val_dataset_CV[fold]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "        # Create the model\n",
    "        model = Network_cof_variation(input_size, hidden_sizes, output_size)\n",
    "        criterion = end_to_end_loss_NN_betaNLL\n",
    "        criterion_val = end_to_end_loss_NN_NLL_cov\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        warmup = True # Define this for loss function to use warmup or not\n",
    "        # Freeze weights for coefficient of variation layer before warm-up epochs\n",
    "        for param in model.coe_var_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        \n",
    "        # Training loop\n",
    "        num_epochs = 3000\n",
    "        warmup_epochs_early_stopping = warmup_epochs + 100  # Early stopping warm-up epochs\n",
    "        early_stopping = EarlyStopping(patience=10, min_delta=1e-4)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch == warmup_epochs:\n",
    "                warmup = False\n",
    "                for param in model.coe_var_layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                inputs, labels = batch\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels, beta, a, warmup, b_weight)\n",
    "                loss_val = criterion_val(outputs, labels, a, warmup, b_weight)\n",
    "\n",
    "                loss.backward()\n",
    "                if epoch >= warmup_epochs:\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                train_loss += loss_val.item()\n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion_val(outputs, targets, a, warmup, b_weight)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            \n",
    "            # Check early stopping condition after warmup\n",
    "            if epoch > warmup_epochs_early_stopping:\n",
    "                early_stopping(val_loss)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "        val_loss_CV.append(val_loss)\n",
    "        model_state_dict[fold] = model.state_dict()\n",
    "    \n",
    "    val_loss_overall = np.mean(val_loss_CV)\n",
    "    print(f\"Validation loss: {val_loss_CV}\")\n",
    "    # Check if the current validation loss is the best\n",
    "    if val_loss_overall < best_val_loss:\n",
    "        best_val_loss = val_loss_overall\n",
    "        # Save the best model\n",
    "        torch.save({'model_state':model_state_dict,\n",
    "                    'lr':lr,\n",
    "                    'num_neuron':num_neuron,\n",
    "                    'num_layer':num_layer,\n",
    "                    'batch_size':batch_size,\n",
    "                    'weight_decay':weight_decay,\n",
    "                    'warmup_epochs':warmup_epochs}, best_model_path)\n",
    "\n",
    "    return val_loss_overall\n",
    "\n",
    "# Create an Optuna study\n",
    "sampler = optuna.samplers.TPESampler(seed=seed_)  # Make the sampler deterministic\n",
    "study = optuna.create_study(sampler=sampler, direction='minimize')\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1: Single model for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dict = torch.load(best_model_path)\n",
    "hidden_sizes = [best_model_dict['num_neuron']]*best_model_dict['num_layer']\n",
    "\n",
    "input_size = 10\n",
    "output_size = 4\n",
    "# Test features as tensor\n",
    "X_train_PCA_tensor = torch.tensor(X_train_PCA,dtype = torch.float32)\n",
    "X_test_in_PCA_tensor = torch.tensor(X_test_in_PCA,dtype=torch.float32)\n",
    "X_test_out_PCA_tensor = torch.tensor(X_test_out_PCA,dtype=torch.float32)\n",
    "\n",
    "b_train_pred = []\n",
    "b_test_in_pred = []\n",
    "b_test_out_pred = []\n",
    "\n",
    "Q_train_pred = []\n",
    "Q_test_in_pred = []\n",
    "Q_test_out_pred = []\n",
    "\n",
    "cov_train_pred = []\n",
    "cov_test_in_pred = []\n",
    "cov_test_out_pred = []\n",
    "\n",
    "for fold in range(10):\n",
    "    # Load the model for the current fold\n",
    "    model = Network_cof_variation(input_size,hidden_sizes,output_size)\n",
    "    model.load_state_dict(best_model_dict['model_state'][fold])\n",
    "\n",
    "    # Evaluate the model with the completed training set and test sets\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        b_train_pred_fold = model(X_train_PCA_tensor).numpy()\n",
    "        b_test_in_pred_fold = model(X_test_in_PCA_tensor).numpy()\n",
    "        b_test_out_pred_fold = model(X_test_out_PCA_tensor).numpy()\n",
    "\n",
    "    Q_train_pred_fold = []\n",
    "    cov_train_pred_fold = []\n",
    "    for i in range(num_training_cells):\n",
    "        b = b_train_pred_fold[i,:3]\n",
    "        cov = b_train_pred_fold[i,3]\n",
    "        Q_train_pred_fold.append(np.clip(empirical_model(a,*b,N_train[i],b_weight),0,1))\n",
    "        cov_train_pred_fold.append(cov)\n",
    "    \n",
    "    Q_test_in_pred_fold = []\n",
    "    cov_test_in_pred_fold = []\n",
    "    for i in range(num_test_in_cells):\n",
    "        b = b_test_in_pred_fold[i,:3]\n",
    "        cov = b_test_in_pred_fold[i,3]\n",
    "        Q_test_in_pred_fold.append(np.clip(empirical_model(a,*b,N_test_in[i],b_weight),0,1))\n",
    "        cov_test_in_pred_fold.append(cov)\n",
    "\n",
    "    Q_test_out_pred_fold = []\n",
    "    cov_test_out_pred_fold = []\n",
    "    for i in range(num_test_out_cells):\n",
    "        b = b_test_out_pred_fold[i,:3]\n",
    "        cov = b_test_out_pred_fold[i,3]\n",
    "        Q_test_out_pred_fold.append(np.clip(empirical_model(a,*b,N_test_out[i],b_weight),0,1))\n",
    "        cov_test_out_pred_fold.append(cov)\n",
    "\n",
    "    # Save the results from the current fold\n",
    "    b_train_pred.append(np.array(b_train_pred_fold))\n",
    "    b_test_in_pred.append(np.array(b_test_in_pred_fold))\n",
    "    b_test_out_pred.append(np.array(b_test_out_pred_fold))\n",
    "\n",
    "    Q_train_pred.append(np.array(Q_train_pred_fold))\n",
    "    Q_test_in_pred.append(np.array(Q_test_in_pred_fold))\n",
    "    Q_test_out_pred.append(np.array(Q_test_out_pred_fold))\n",
    "\n",
    "    cov_train_pred.append(np.array(cov_train_pred_fold))\n",
    "    cov_test_in_pred.append(np.array(cov_test_in_pred_fold))\n",
    "    cov_test_out_pred.append(np.array(cov_test_out_pred_fold))\n",
    "\n",
    "\n",
    "# Write the results to files\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/b_train_pred_1.pkl','wb') as f:\n",
    "    pickle.dump(b_train_pred,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/b_test_in_pred_1.pkl','wb') as f:\n",
    "    pickle.dump(b_test_in_pred,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/b_test_out_pred_1.pkl','wb') as f:\n",
    "    pickle.dump(b_test_out_pred,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_train_pred_1.pkl','wb') as f:\n",
    "    pickle.dump(Q_train_pred,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_test_in_pred_1.pkl','wb') as f:\n",
    "    pickle.dump(Q_test_in_pred,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_test_out_pred_1.pkl','wb') as f:\n",
    "    pickle.dump(Q_test_out_pred,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/cov_train_pred_1.pkl','wb') as f:\n",
    "    pickle.dump(cov_train_pred,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/cov_test_in_pred_1.pkl','wb') as f:\n",
    "    pickle.dump(cov_test_in_pred,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/cov_test_out_pred_1.pkl','wb') as f:\n",
    "    pickle.dump(cov_test_out_pred,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE training: 1.85\n",
      "Mean MAE test in: 2.43\n",
      "Mean MAE test out: 7.84\n",
      "Mean RMSE training: 2.24\n",
      "Mean RMSE test in: 2.97\n",
      "Mean RMSE test out: 9.96\n"
     ]
    }
   ],
   "source": [
    "MAE_training = []\n",
    "MAE_test_in = []\n",
    "MAE_test_out = []\n",
    "\n",
    "RMSE_training = []\n",
    "RMSE_test_in = []\n",
    "RMSE_test_out = []\n",
    "\n",
    "for fold in range(10):\n",
    "    MAE_training_fold = []\n",
    "    MAE_test_in_fold = []\n",
    "    MAE_test_out_fold = []\n",
    "\n",
    "    RMSE_training_fold = []\n",
    "    RMSE_test_in_fold = []\n",
    "    RMSE_test_out_fold = []\n",
    "\n",
    "    for i in range(num_training_cells):\n",
    "        MAE_training_fold.append(mean_absolute_error(Q_train[i],Q_train_pred[fold][i]))\n",
    "        RMSE_training_fold.append(root_mean_squared_error(Q_train[i],Q_train_pred[fold][i]))\n",
    "\n",
    "    for i in range(num_test_in_cells):\n",
    "        MAE_test_in_fold.append(mean_absolute_error(Q_test_in[i],Q_test_in_pred[fold][i]))\n",
    "        RMSE_test_in_fold.append(root_mean_squared_error(Q_test_in[i],Q_test_in_pred[fold][i]))\n",
    "\n",
    "    for i in range(num_test_out_cells):\n",
    "        MAE_test_out_fold.append(mean_absolute_error(Q_test_out[i],Q_test_out_pred[fold][i]))\n",
    "        RMSE_test_out_fold.append(root_mean_squared_error(Q_test_out[i],Q_test_out_pred[fold][i]))\n",
    "\n",
    "    MAE_training.append(np.mean(MAE_training_fold)*100)\n",
    "    MAE_test_in.append(np.mean(MAE_test_in_fold)*100)\n",
    "    MAE_test_out.append(np.mean(MAE_test_out_fold)*100)\n",
    "\n",
    "    RMSE_training.append(np.mean(RMSE_training_fold)*100)\n",
    "    RMSE_test_in.append(np.mean(RMSE_test_in_fold)*100)\n",
    "    RMSE_test_out.append(np.mean(RMSE_test_out_fold)*100)\n",
    "\n",
    "result_dict = {'MAE_training':MAE_training,\n",
    "                'MAE_test_in':MAE_test_in,\n",
    "                'MAE_test_out':MAE_test_out,\n",
    "                'RMSE_training':RMSE_training,\n",
    "                'RMSE_test_in':RMSE_test_in,\n",
    "                'RMSE_test_out':RMSE_test_out}\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/result_dict.pkl','wb') as f:\n",
    "    pickle.dump(result_dict,f)\n",
    "\n",
    "mean_MAE_training = np.mean(MAE_training)\n",
    "mean_MAE_test_in = np.mean(MAE_test_in)\n",
    "mean_MAE_test_out = np.mean(MAE_test_out)\n",
    "\n",
    "mean_RMSE_training = np.mean(RMSE_training)\n",
    "mean_RMSE_test_in = np.mean(RMSE_test_in)\n",
    "mean_RMSE_test_out = np.mean(RMSE_test_out)\n",
    "\n",
    "print(f\"Mean MAE training: {mean_MAE_training:.2f}\")\n",
    "print(f\"Mean MAE test in: {mean_MAE_test_in:.2f}\")\n",
    "print(f\"Mean MAE test out: {mean_MAE_test_out:.2f}\")\n",
    "\n",
    "print(f\"Mean RMSE training: {mean_RMSE_training:.2f}\")\n",
    "print(f\"Mean RMSE test in: {mean_RMSE_test_in:.2f}\")\n",
    "print(f\"Mean RMSE test out: {mean_RMSE_test_out:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE training: [1.643545350505334, 2.09967585249802, 1.7676023752392962, 1.974062259304656, 1.8837080791359464, 1.8206665489696234, 1.7522265953285927, 1.8638849341687598, 1.9714080606586553, 1.7470637235898319]\n",
      "MAE test in: [2.3389947311835124, 2.4640858057345683, 2.363188725121361, 2.564984971141379, 3.4211871588080895, 2.102207843626132, 2.2165959025676902, 1.986713193687822, 2.4934434410148967, 2.3846918264725514]\n",
      "MAE test out: [12.384958694318204, 7.712989836741765, 4.418280810455669, 13.134221757590508, 6.314898340426757, 3.045295932947437, 4.014358531770969, 5.434284287702664, 11.997625725366785, 9.924846708968296]\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAE training: {MAE_training}\")\n",
    "print(f\"MAE test in: {MAE_test_in}\")\n",
    "print(f\"MAE test out: {MAE_test_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Ensemble of 5 models for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 779\n",
      "Fold 1, Model 1, Validation loss: 3.8215935230255127\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 2, Validation loss: 2.4502148628234863\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 3, Validation loss: 3.09334397315979\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 4, Validation loss: 3.1151938438415527\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 5, Validation loss: 2.4279656410217285\n",
      "Early stopping at epoch 779\n",
      "Fold 2, Model 1, Validation loss: 2.0148041248321533\n",
      "Early stopping at epoch 779\n",
      "Fold 2, Model 2, Validation loss: 3.1719396114349365\n",
      "Early stopping at epoch 779\n",
      "Fold 2, Model 3, Validation loss: 1.9256962537765503\n",
      "Early stopping at epoch 852\n",
      "Fold 2, Model 4, Validation loss: 3.7551651000976562\n",
      "Early stopping at epoch 779\n",
      "Fold 2, Model 5, Validation loss: 12.877677917480469\n",
      "Early stopping at epoch 779\n",
      "Fold 3, Model 1, Validation loss: 1.2635449171066284\n",
      "Early stopping at epoch 779\n",
      "Fold 3, Model 2, Validation loss: 1.644436001777649\n",
      "Early stopping at epoch 779\n",
      "Fold 3, Model 3, Validation loss: 1.2860348224639893\n",
      "Early stopping at epoch 779\n",
      "Fold 3, Model 4, Validation loss: 1.6514911651611328\n",
      "Early stopping at epoch 780\n",
      "Fold 3, Model 5, Validation loss: 1.4829585552215576\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 1, Validation loss: 2.6224262714385986\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 2, Validation loss: 3.2171363830566406\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 3, Validation loss: 2.915923833847046\n",
      "Early stopping at epoch 980\n",
      "Fold 4, Model 4, Validation loss: 2.2469255924224854\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 5, Validation loss: 1.9876363277435303\n",
      "Early stopping at epoch 847\n",
      "Fold 5, Model 1, Validation loss: 1.5701463222503662\n",
      "Early stopping at epoch 831\n",
      "Fold 5, Model 2, Validation loss: 1.4180207252502441\n",
      "Early stopping at epoch 779\n",
      "Fold 5, Model 3, Validation loss: 1.6119349002838135\n",
      "Early stopping at epoch 779\n",
      "Fold 5, Model 4, Validation loss: 1.7649149894714355\n",
      "Early stopping at epoch 779\n",
      "Fold 5, Model 5, Validation loss: 1.9871816635131836\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 1, Validation loss: 1.9269291162490845\n",
      "Early stopping at epoch 876\n",
      "Fold 6, Model 2, Validation loss: 1.6572147607803345\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 3, Validation loss: 2.0469658374786377\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 4, Validation loss: 1.9003686904907227\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 5, Validation loss: 4.287543296813965\n",
      "Early stopping at epoch 779\n",
      "Fold 7, Model 1, Validation loss: 1.6528513431549072\n",
      "Early stopping at epoch 803\n",
      "Fold 7, Model 2, Validation loss: 1.3175643682479858\n",
      "Early stopping at epoch 892\n",
      "Fold 7, Model 3, Validation loss: 1.4893349409103394\n",
      "Early stopping at epoch 1179\n",
      "Fold 7, Model 4, Validation loss: 1.2276384830474854\n",
      "Early stopping at epoch 779\n",
      "Fold 7, Model 5, Validation loss: 7.178215503692627\n",
      "Early stopping at epoch 950\n",
      "Fold 8, Model 1, Validation loss: 1.3677167892456055\n",
      "Early stopping at epoch 974\n",
      "Fold 8, Model 2, Validation loss: 0.9371445178985596\n",
      "Early stopping at epoch 891\n",
      "Fold 8, Model 3, Validation loss: 1.0125230550765991\n",
      "Early stopping at epoch 870\n",
      "Fold 8, Model 4, Validation loss: 1.3644462823867798\n",
      "Early stopping at epoch 852\n",
      "Fold 8, Model 5, Validation loss: 1.0685889720916748\n",
      "Early stopping at epoch 880\n",
      "Fold 9, Model 1, Validation loss: 1.379652500152588\n",
      "Early stopping at epoch 876\n",
      "Fold 9, Model 2, Validation loss: 1.4201205968856812\n",
      "Early stopping at epoch 1782\n",
      "Fold 9, Model 3, Validation loss: 1.0450496673583984\n",
      "Early stopping at epoch 779\n",
      "Fold 9, Model 4, Validation loss: 1.6346279382705688\n",
      "Early stopping at epoch 779\n",
      "Fold 9, Model 5, Validation loss: 1.3722999095916748\n",
      "Early stopping at epoch 850\n",
      "Fold 10, Model 1, Validation loss: 1.1718829870224\n",
      "Early stopping at epoch 849\n",
      "Fold 10, Model 2, Validation loss: 1.1444263458251953\n",
      "Early stopping at epoch 800\n",
      "Fold 10, Model 3, Validation loss: 1.1868444681167603\n",
      "Early stopping at epoch 1043\n",
      "Fold 10, Model 4, Validation loss: 1.0042805671691895\n",
      "Early stopping at epoch 903\n",
      "Fold 10, Model 5, Validation loss: 1.3441545963287354\n"
     ]
    }
   ],
   "source": [
    "# Load the hyperparameters for the best model\n",
    "hidden_sizes = [best_model_dict['num_neuron']]*best_model_dict['num_layer']\n",
    "lr = best_model_dict['lr']\n",
    "batch_size = best_model_dict['batch_size']\n",
    "weight_decay = best_model_dict['weight_decay']\n",
    "warmup_epochs = best_model_dict['warmup_epochs']\n",
    "# Define the warm-up epochs for early stopping\n",
    "warmup_epochs_early_stopping = warmup_epochs + 100\n",
    "num_epochs = 3000\n",
    "# Number of models to train for ensemble\n",
    "M = 5\n",
    "# Beta value for Beta-NLL loss\n",
    "beta = 0.5\n",
    "# Gradient clipping value\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "model_state_dict_5 = {}\n",
    "\n",
    "for fold in range(10):\n",
    "    model_state_dict_5[fold] = []\n",
    "    val_loss_fold = []\n",
    "    for m in range(M):\n",
    "        model = Network_cof_variation(input_size, hidden_sizes, output_size)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = end_to_end_loss_NN_betaNLL\n",
    "        criterion_val = end_to_end_loss_NN_NLL_cov\n",
    "        early_stopping = EarlyStopping(patience=10, min_delta=1e-4)\n",
    "        # Freeze weights for coefficient of variation layer before warm-up epochs\n",
    "        warmup = True\n",
    "        for param in model.coe_var_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        train_dataset = train_dataset_CV[fold]\n",
    "        val_dataset = val_dataset_CV[fold]\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch == warmup_epochs:\n",
    "                warmup = False\n",
    "                for param in model.coe_var_layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                inputs, labels = batch\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels, beta, a, warmup, b_weight)\n",
    "                loss_val = criterion_val(outputs, labels, a, warmup, b_weight)\n",
    "                loss.backward()\n",
    "                if epoch >= warmup_epochs:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                train_loss += loss_val.item()\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion_val(outputs, targets, a, warmup, b_weight)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if epoch > warmup_epochs_early_stopping:\n",
    "                early_stopping(val_loss)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        \n",
    "        model_state_dict_5[fold].append(model.state_dict())\n",
    "        val_loss_fold.append(val_loss)\n",
    "        print(f\"Fold {fold + 1}, Model {m + 1}, Validation loss: {val_loss}\")\n",
    "    \n",
    "with open('Best_network/best_NN_NLL_cov_V2_5.pth','wb') as f:\n",
    "    torch.save({'model_state':model_state_dict_5,\n",
    "                'lr':lr,\n",
    "                'num_neuron':best_model_dict['num_neuron'],\n",
    "                'num_layer':best_model_dict['num_layer'],\n",
    "                'batch_size':batch_size,\n",
    "                'weight_decay':weight_decay,\n",
    "                'warmup_epochs':warmup_epochs},f)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train_pred_5_all = {}\n",
    "b_test_in_pred_5_all = {}\n",
    "b_test_out_pred_5_all = {}\n",
    "\n",
    "for fold in range(10):\n",
    "    b_train_pred_5 = []\n",
    "    b_test_in_pred_5 = []\n",
    "    b_test_out_pred_5 = []\n",
    "\n",
    "    for m in range(M):\n",
    "        model = Network_cof_variation(input_size, hidden_sizes, output_size)\n",
    "        model.load_state_dict(model_state_dict_5[fold][m])\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            b_train_pred_fold_m = model(X_train_PCA_tensor).numpy()\n",
    "            b_test_in_pred_fold_m = model(X_test_in_PCA_tensor).numpy()\n",
    "            b_test_out_pred_fold_m = model(X_test_out_PCA_tensor).numpy()\n",
    "\n",
    "        b_train_pred_5.append(b_train_pred_fold_m)\n",
    "        b_test_in_pred_5.append(b_test_in_pred_fold_m)\n",
    "        b_test_out_pred_5.append(b_test_out_pred_fold_m)\n",
    "\n",
    "    b_train_pred_5_all[fold] = np.array(b_train_pred_5)\n",
    "    b_test_in_pred_5_all[fold] = np.array(b_test_in_pred_5)\n",
    "    b_test_out_pred_5_all[fold] = np.array(b_test_out_pred_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Empirical_parameter_results/E2E_NNE_V2/b_train_pred_5_all.pkl','wb') as f:\n",
    "    pickle.dump(b_train_pred_5_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/b_test_in_pred_5_all.pkl','wb') as f:\n",
    "    pickle.dump(b_test_in_pred_5_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/b_test_out_pred_5_all.pkl','wb') as f:\n",
    "    pickle.dump(b_test_out_pred_5_all,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_train_pred_5_all = {}\n",
    "Q_test_in_pred_5_all = {}\n",
    "Q_test_out_pred_5_all = {}\n",
    "\n",
    "std_train_pred_5_all = {}\n",
    "std_test_in_pred_5_all = {}\n",
    "std_test_out_pred_5_all = {}\n",
    "\n",
    "Q_train_pred_5_ensemble = {}\n",
    "Q_test_in_pred_5_ensemble = {}\n",
    "Q_test_out_pred_5_ensemble = {}\n",
    "\n",
    "std_train_pred_5_ensemble = {}\n",
    "std_test_in_pred_5_ensemble = {}\n",
    "std_test_out_pred_5_ensemble = {}\n",
    "\n",
    "for fold in range(10):\n",
    "    Q_train_hat_all_models_fold = np.array([[empirical_model(a, b1, b2, b3, N_train[i],b_weight)*100 for i, (b1, b2, b3, _) in enumerate(model_b_train_hat)] for model_b_train_hat in b_train_pred_5_all[fold]])\n",
    "    Q_test_in_hat_all_models_fold = np.array([[empirical_model(a, b1, b2, b3, N_test_in[i],b_weight)*100 for i, (b1, b2, b3, _) in enumerate(model_b_test_in_hat)] for model_b_test_in_hat in b_test_in_pred_5_all[fold]])\n",
    "    Q_test_out_hat_all_models_fold = np.array([[empirical_model(a, b1, b2, b3, N_test_out[i],b_weight)*100 for i, (b1, b2, b3, _) in enumerate(model_b_test_out_hat)] for model_b_test_out_hat in b_test_out_pred_5_all[fold]])\n",
    "\n",
    "    coe_var_train_all_models_fold = np.array([np.repeat(model_b_train_hat[:, 3][:, np.newaxis], Q_train.shape[1], axis=1) for model_b_train_hat in b_train_pred_5_all[fold]])\n",
    "    coe_var_test_in_all_models_fold = np.array([np.repeat(model_b_test_in_hat[:, 3][:, np.newaxis], Q_test_in.shape[1], axis=1) for model_b_test_in_hat in b_test_in_pred_5_all[fold]])\n",
    "    coe_var_test_out_all_models_fold = np.array([np.repeat(model_b_test_out_hat[:, 3][:, np.newaxis], Q_test_out.shape[1], axis=1) for model_b_test_out_hat in b_test_out_pred_5_all[fold]])\n",
    "\n",
    "    # Calculate the variance for each individual model\n",
    "    std_train_all_models_fold = coe_var_train_all_models_fold * (100 - Q_train_hat_all_models_fold)\n",
    "    std_test_in_all_models_fold = coe_var_test_in_all_models_fold * (100 - Q_test_in_hat_all_models_fold)\n",
    "    std_test_out_all_models_fold = coe_var_test_out_all_models_fold * (100 - Q_test_out_hat_all_models_fold)\n",
    "\n",
    "    var_train_all_models_fold = std_train_all_models_fold ** 2\n",
    "    var_test_in_all_models_fold = std_test_in_all_models_fold ** 2\n",
    "    var_test_out_all_models_fold = std_test_out_all_models_fold ** 2\n",
    "\n",
    "    Q_train_hat_combined_fold = np.mean(Q_train_hat_all_models_fold, axis=0)\n",
    "    Q_test_in_hat_combined_fold = np.mean(Q_test_in_hat_all_models_fold, axis=0)\n",
    "    Q_test_out_hat_combined_fold = np.mean(Q_test_out_hat_all_models_fold, axis=0)\n",
    "\n",
    "    # Calculate the variance of the combined predictions\n",
    "    var_train_combined_fold = np.mean(var_train_all_models_fold + Q_train_hat_all_models_fold**2 - Q_train_hat_combined_fold**2, axis=0)\n",
    "    var_test_in_combined_fold = np.mean(var_test_in_all_models_fold + Q_test_in_hat_all_models_fold**2 - Q_test_in_hat_combined_fold**2, axis=0)\n",
    "    var_test_out_combined_fold = np.mean(var_test_out_all_models_fold + Q_test_out_hat_all_models_fold**2 - Q_test_out_hat_combined_fold**2, axis=0)\n",
    "\n",
    "    std_train_combined_fold = np.sqrt(var_train_combined_fold)\n",
    "    std_test_in_combined_fold = np.sqrt(var_test_in_combined_fold)\n",
    "    std_test_out_combined_fold = np.sqrt(var_test_out_combined_fold)\n",
    "\n",
    "    Q_train_pred_5_all[fold] = Q_train_hat_all_models_fold\n",
    "    Q_test_in_pred_5_all[fold] = Q_test_in_hat_all_models_fold\n",
    "    Q_test_out_pred_5_all[fold] = Q_test_out_hat_all_models_fold\n",
    "\n",
    "    std_train_pred_5_all[fold] = std_train_all_models_fold\n",
    "    std_test_in_pred_5_all[fold] = std_test_in_all_models_fold\n",
    "    std_test_out_pred_5_all[fold] = std_test_out_all_models_fold\n",
    "\n",
    "    Q_train_pred_5_ensemble[fold] = Q_train_hat_combined_fold\n",
    "    Q_test_in_pred_5_ensemble[fold] = Q_test_in_hat_combined_fold\n",
    "    Q_test_out_pred_5_ensemble[fold] = Q_test_out_hat_combined_fold\n",
    "\n",
    "    std_train_pred_5_ensemble[fold] = std_train_combined_fold\n",
    "    std_test_in_pred_5_ensemble[fold] = std_test_in_combined_fold\n",
    "    std_test_out_pred_5_ensemble[fold] = std_test_out_combined_fold\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_train_pred_5_all.pkl','wb') as f:\n",
    "    pickle.dump(Q_train_pred_5_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_test_in_pred_5_all.pkl','wb') as f:\n",
    "    pickle.dump(Q_test_in_pred_5_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_test_out_pred_5_all.pkl','wb') as f:\n",
    "    pickle.dump(Q_test_out_pred_5_all,f)\n",
    "\n",
    "with open('Empirical-parameter_results/E2E_NNE_V2/std_train_pred_5_all.pkl','wb') as f:\n",
    "    pickle.dump(std_train_pred_5_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_test_in_pred_5_all.pkl','wb') as f:\n",
    "    pickle.dump(std_test_in_pred_5_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_test_out_pred_5_all.pkl','wb') as f:\n",
    "    pickle.dump(std_test_out_pred_5_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_train_pred_5_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(Q_train_pred_5_ensemble,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_test_in_pred_5_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(Q_test_in_pred_5_ensemble,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_test_out_pred_5_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(Q_test_out_pred_5_ensemble,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_train_pred_5_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(std_train_pred_5_ensemble,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_test_in_pred_5_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(std_test_in_pred_5_ensemble,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_test_out_pred_5_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(std_test_out_pred_5_ensemble,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE training: 1.70\n",
      "Mean MAE test in: 2.00\n",
      "Mean MAE test out: 6.44\n",
      "Mean RMSE training: 2.10\n",
      "Mean RMSE test in: 2.45\n",
      "Mean RMSE test out: 8.05\n"
     ]
    }
   ],
   "source": [
    "MAE_training = []\n",
    "MAE_test_in = []\n",
    "MAE_test_out = []\n",
    "\n",
    "RMSE_training = []\n",
    "RMSE_test_in = []\n",
    "RMSE_test_out = []\n",
    "\n",
    "for fold in range(10):\n",
    "    MAE_training_fold = []\n",
    "    MAE_test_in_fold = []\n",
    "    MAE_test_out_fold = []\n",
    "\n",
    "    RMSE_training_fold = []\n",
    "    RMSE_test_in_fold = []\n",
    "    RMSE_test_out_fold = []\n",
    "\n",
    "    for i in range(num_training_cells):\n",
    "        MAE_training_fold.append(mean_absolute_error(Q_train[i]*100,Q_train_pred_5_ensemble[fold][i]))\n",
    "        RMSE_training_fold.append(root_mean_squared_error(Q_train[i]*100,Q_train_pred_5_ensemble[fold][i]))\n",
    "\n",
    "    for i in range(num_test_in_cells):\n",
    "        MAE_test_in_fold.append(mean_absolute_error(Q_test_in[i]*100,Q_test_in_pred_5_ensemble[fold][i]))\n",
    "        RMSE_test_in_fold.append(root_mean_squared_error(Q_test_in[i]*100,Q_test_in_pred_5_ensemble[fold][i]))\n",
    "\n",
    "    for i in range(num_test_out_cells):\n",
    "        MAE_test_out_fold.append(mean_absolute_error(Q_test_out[i]*100,Q_test_out_pred_5_ensemble[fold][i]))\n",
    "        RMSE_test_out_fold.append(root_mean_squared_error(Q_test_out[i]*100,Q_test_out_pred_5_ensemble[fold][i]))\n",
    "\n",
    "    MAE_training.append(np.mean(MAE_training_fold))\n",
    "    MAE_test_in.append(np.mean(MAE_test_in_fold))\n",
    "    MAE_test_out.append(np.mean(MAE_test_out_fold))\n",
    "\n",
    "    RMSE_training.append(np.mean(RMSE_training_fold))\n",
    "    RMSE_test_in.append(np.mean(RMSE_test_in_fold))\n",
    "    RMSE_test_out.append(np.mean(RMSE_test_out_fold))\n",
    "\n",
    "result_dict = {'MAE_training':MAE_training,\n",
    "                'MAE_test_in':MAE_test_in,\n",
    "                'MAE_test_out':MAE_test_out,\n",
    "                'RMSE_training':RMSE_training,\n",
    "                'RMSE_test_in':RMSE_test_in,\n",
    "                'RMSE_test_out':RMSE_test_out}\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/result_dict_5.pkl','wb') as f:\n",
    "    pickle.dump(result_dict,f)\n",
    "\n",
    "mean_MAE_training = np.mean(MAE_training)\n",
    "mean_MAE_test_in = np.mean(MAE_test_in)\n",
    "mean_MAE_test_out = np.mean(MAE_test_out)\n",
    "\n",
    "mean_RMSE_training = np.mean(RMSE_training)\n",
    "mean_RMSE_test_in = np.mean(RMSE_test_in)\n",
    "mean_RMSE_test_out = np.mean(RMSE_test_out)\n",
    "\n",
    "print(f\"Mean MAE training: {mean_MAE_training:.2f}\")\n",
    "print(f\"Mean MAE test in: {mean_MAE_test_in:.2f}\")\n",
    "print(f\"Mean MAE test out: {mean_MAE_test_out:.2f}\")\n",
    "\n",
    "print(f\"Mean RMSE training: {mean_RMSE_training:.2f}\")\n",
    "print(f\"Mean RMSE test in: {mean_RMSE_test_in:.2f}\")\n",
    "print(f\"Mean RMSE test out: {mean_RMSE_test_out:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE training: [1.78373181841315, 1.6457707954245309, 1.6905657949088835, 1.6914115805063543, 1.6920497789455553, 1.7432345106643212, 1.689637178767997, 1.6798989018940038, 1.6583976309139477, 1.7403471134382769]\n",
      "MAE test in: [2.0343837807806238, 1.9400271014985158, 2.2426159246530264, 2.0262238336288414, 2.0114062041676566, 1.9822969096112968, 2.001414718506352, 2.0507026018673056, 1.9423711434600974, 1.7585559089400231]\n",
      "MAE test out: [4.658718441482848, 7.166958282987388, 9.293466977805746, 4.6115828530037035, 5.957440617093438, 4.959447638833681, 6.003882856250859, 8.961618578398028, 7.003241850328285, 5.793628848759678]\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAE training: {MAE_training}\")\n",
    "print(f\"MAE test in: {MAE_test_in}\")\n",
    "print(f\"MAE test out: {MAE_test_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3: Ensemble of 10 models for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 779\n",
      "Fold 1, Model 1, Validation loss: 3.361151933670044\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 2, Validation loss: 2.5899298191070557\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 3, Validation loss: 2.2884302139282227\n",
      "Early stopping at epoch 780\n",
      "Fold 1, Model 4, Validation loss: 2.742144823074341\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 5, Validation loss: 6.782578468322754\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 6, Validation loss: 2.1262478828430176\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 7, Validation loss: 3.5219531059265137\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 8, Validation loss: 2.6193976402282715\n",
      "Early stopping at epoch 790\n",
      "Fold 1, Model 9, Validation loss: 2.7740678787231445\n",
      "Early stopping at epoch 779\n",
      "Fold 1, Model 10, Validation loss: 8.166481018066406\n",
      "Early stopping at epoch 876\n",
      "Fold 2, Model 1, Validation loss: 3.3756256103515625\n",
      "Early stopping at epoch 1114\n",
      "Fold 2, Model 2, Validation loss: 3.0240554809570312\n",
      "Early stopping at epoch 779\n",
      "Fold 2, Model 3, Validation loss: 3.185882568359375\n",
      "Early stopping at epoch 779\n",
      "Fold 2, Model 4, Validation loss: 2.552511215209961\n",
      "Early stopping at epoch 779\n",
      "Fold 2, Model 5, Validation loss: 4.755248546600342\n",
      "Early stopping at epoch 861\n",
      "Fold 2, Model 6, Validation loss: 4.6783905029296875\n",
      "Early stopping at epoch 779\n",
      "Fold 2, Model 7, Validation loss: 2.489804267883301\n",
      "Early stopping at epoch 779\n",
      "Fold 2, Model 8, Validation loss: 2.1186575889587402\n",
      "Early stopping at epoch 779\n",
      "Fold 2, Model 9, Validation loss: 4.266711235046387\n",
      "Early stopping at epoch 817\n",
      "Fold 2, Model 10, Validation loss: 1.9720426797866821\n",
      "Early stopping at epoch 1107\n",
      "Fold 3, Model 1, Validation loss: 1.4844633340835571\n",
      "Early stopping at epoch 779\n",
      "Fold 3, Model 2, Validation loss: 2.024613380432129\n",
      "Early stopping at epoch 1027\n",
      "Fold 3, Model 3, Validation loss: 1.0026185512542725\n",
      "Early stopping at epoch 993\n",
      "Fold 3, Model 4, Validation loss: 1.2434204816818237\n",
      "Early stopping at epoch 909\n",
      "Fold 3, Model 5, Validation loss: 0.9498798847198486\n",
      "Early stopping at epoch 835\n",
      "Fold 3, Model 6, Validation loss: 1.17666757106781\n",
      "Early stopping at epoch 786\n",
      "Fold 3, Model 7, Validation loss: 1.5989829301834106\n",
      "Early stopping at epoch 813\n",
      "Fold 3, Model 8, Validation loss: 1.3119200468063354\n",
      "Early stopping at epoch 779\n",
      "Fold 3, Model 9, Validation loss: 1.4668869972229004\n",
      "Early stopping at epoch 799\n",
      "Fold 3, Model 10, Validation loss: 1.344379186630249\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 1, Validation loss: 2.3935532569885254\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 2, Validation loss: 4.693365097045898\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 3, Validation loss: 2.565028667449951\n",
      "Early stopping at epoch 877\n",
      "Fold 4, Model 4, Validation loss: 1.6550098657608032\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 5, Validation loss: 2.3002262115478516\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 6, Validation loss: 7.83953332901001\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 7, Validation loss: 2.7383742332458496\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 8, Validation loss: 4.265683174133301\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 9, Validation loss: 2.6561596393585205\n",
      "Early stopping at epoch 779\n",
      "Fold 4, Model 10, Validation loss: 2.476179838180542\n",
      "Early stopping at epoch 785\n",
      "Fold 5, Model 1, Validation loss: 1.5349546670913696\n",
      "Early stopping at epoch 779\n",
      "Fold 5, Model 2, Validation loss: 1.2793587446212769\n",
      "Early stopping at epoch 806\n",
      "Fold 5, Model 3, Validation loss: 1.6772363185882568\n",
      "Early stopping at epoch 806\n",
      "Fold 5, Model 4, Validation loss: 1.3405225276947021\n",
      "Early stopping at epoch 851\n",
      "Fold 5, Model 5, Validation loss: 2.0214412212371826\n",
      "Early stopping at epoch 779\n",
      "Fold 5, Model 6, Validation loss: 2.160245895385742\n",
      "Early stopping at epoch 779\n",
      "Fold 5, Model 7, Validation loss: 1.4600069522857666\n",
      "Early stopping at epoch 780\n",
      "Fold 5, Model 8, Validation loss: 1.8802231550216675\n",
      "Early stopping at epoch 794\n",
      "Fold 5, Model 9, Validation loss: 1.5061521530151367\n",
      "Early stopping at epoch 799\n",
      "Fold 5, Model 10, Validation loss: 1.375295639038086\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 1, Validation loss: 1.925650954246521\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 2, Validation loss: 3.8726773262023926\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 3, Validation loss: 2.1222238540649414\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 4, Validation loss: 1.9892367124557495\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 5, Validation loss: 2.0003321170806885\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 6, Validation loss: 2.223456382751465\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 7, Validation loss: 1.609979510307312\n",
      "Early stopping at epoch 1041\n",
      "Fold 6, Model 8, Validation loss: 3.0519540309906006\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 9, Validation loss: 2.4748637676239014\n",
      "Early stopping at epoch 779\n",
      "Fold 6, Model 10, Validation loss: 1.834526777267456\n",
      "Early stopping at epoch 784\n",
      "Fold 7, Model 1, Validation loss: 1.4776935577392578\n",
      "Early stopping at epoch 878\n",
      "Fold 7, Model 2, Validation loss: 1.3055120706558228\n",
      "Early stopping at epoch 821\n",
      "Fold 7, Model 3, Validation loss: 1.3841931819915771\n",
      "Early stopping at epoch 1200\n",
      "Fold 7, Model 4, Validation loss: 1.0585726499557495\n",
      "Early stopping at epoch 880\n",
      "Fold 7, Model 5, Validation loss: 1.1565059423446655\n",
      "Early stopping at epoch 1140\n",
      "Fold 7, Model 6, Validation loss: 1.1468837261199951\n",
      "Early stopping at epoch 834\n",
      "Fold 7, Model 7, Validation loss: 1.3545095920562744\n",
      "Early stopping at epoch 1090\n",
      "Fold 7, Model 8, Validation loss: 1.0454283952713013\n",
      "Early stopping at epoch 804\n",
      "Fold 7, Model 9, Validation loss: 1.2331310510635376\n",
      "Early stopping at epoch 972\n",
      "Fold 7, Model 10, Validation loss: 1.3423676490783691\n",
      "Early stopping at epoch 946\n",
      "Fold 8, Model 1, Validation loss: 1.4336566925048828\n",
      "Early stopping at epoch 810\n",
      "Fold 8, Model 2, Validation loss: 0.9722253084182739\n",
      "Early stopping at epoch 1045\n",
      "Fold 8, Model 3, Validation loss: 1.24569833278656\n",
      "Early stopping at epoch 984\n",
      "Fold 8, Model 4, Validation loss: 0.908122181892395\n",
      "Early stopping at epoch 780\n",
      "Fold 8, Model 5, Validation loss: 1.732248067855835\n",
      "Early stopping at epoch 1185\n",
      "Fold 8, Model 6, Validation loss: 1.0506106615066528\n",
      "Early stopping at epoch 1192\n",
      "Fold 8, Model 7, Validation loss: 1.1352475881576538\n",
      "Early stopping at epoch 779\n",
      "Fold 8, Model 8, Validation loss: 2.7409262657165527\n",
      "Early stopping at epoch 1253\n",
      "Fold 8, Model 9, Validation loss: 1.2106066942214966\n",
      "Early stopping at epoch 818\n",
      "Fold 8, Model 10, Validation loss: 1.2808692455291748\n",
      "Early stopping at epoch 1187\n",
      "Fold 9, Model 1, Validation loss: 1.319705605506897\n",
      "Early stopping at epoch 779\n",
      "Fold 9, Model 2, Validation loss: 2.432196617126465\n",
      "Early stopping at epoch 779\n",
      "Fold 9, Model 3, Validation loss: 1.3181391954421997\n",
      "Early stopping at epoch 832\n",
      "Fold 9, Model 4, Validation loss: 1.262164831161499\n",
      "Early stopping at epoch 794\n",
      "Fold 9, Model 5, Validation loss: 1.490278720855713\n",
      "Early stopping at epoch 779\n",
      "Fold 9, Model 6, Validation loss: 19.119537353515625\n",
      "Early stopping at epoch 1274\n",
      "Fold 9, Model 7, Validation loss: 1.1068223714828491\n",
      "Early stopping at epoch 1132\n",
      "Fold 9, Model 8, Validation loss: 1.1301414966583252\n",
      "Early stopping at epoch 901\n",
      "Fold 9, Model 9, Validation loss: 1.4923746585845947\n",
      "Early stopping at epoch 827\n",
      "Fold 9, Model 10, Validation loss: 1.533076286315918\n",
      "Early stopping at epoch 864\n",
      "Fold 10, Model 1, Validation loss: 1.107337474822998\n",
      "Early stopping at epoch 906\n",
      "Fold 10, Model 2, Validation loss: 1.3216822147369385\n",
      "Early stopping at epoch 848\n",
      "Fold 10, Model 3, Validation loss: 1.3343619108200073\n",
      "Early stopping at epoch 979\n",
      "Fold 10, Model 4, Validation loss: 1.273141622543335\n",
      "Early stopping at epoch 1126\n",
      "Fold 10, Model 5, Validation loss: 1.4242029190063477\n",
      "Early stopping at epoch 1011\n",
      "Fold 10, Model 6, Validation loss: 1.123334527015686\n",
      "Early stopping at epoch 1438\n",
      "Fold 10, Model 7, Validation loss: 1.0166987180709839\n",
      "Early stopping at epoch 898\n",
      "Fold 10, Model 8, Validation loss: 1.485082745552063\n",
      "Early stopping at epoch 803\n",
      "Fold 10, Model 9, Validation loss: 1.5570122003555298\n",
      "Early stopping at epoch 779\n",
      "Fold 10, Model 10, Validation loss: 1.582866907119751\n"
     ]
    }
   ],
   "source": [
    "# Load the hyperparameters for the best model\n",
    "hidden_sizes = [best_model_dict['num_neuron']]*best_model_dict['num_layer']\n",
    "lr = best_model_dict['lr']\n",
    "batch_size = best_model_dict['batch_size']\n",
    "weight_decay = best_model_dict['weight_decay']\n",
    "warmup_epochs = best_model_dict['warmup_epochs']\n",
    "# Define the warm-up epochs for early stopping\n",
    "warmup_epochs_early_stopping = warmup_epochs + 100\n",
    "num_epochs = 3000\n",
    "# Number of models to train for ensemble\n",
    "M = 10\n",
    "# Beta value for Beta-NLL loss\n",
    "beta = 0.5\n",
    "# Gradient clipping value\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "model_state_dict_10 = {}\n",
    "\n",
    "for fold in range(10):\n",
    "    model_state_dict_10[fold] = []\n",
    "    val_loss_fold = []\n",
    "    for m in range(M):\n",
    "        model = Network_cof_variation(input_size, hidden_sizes, output_size)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = end_to_end_loss_NN_betaNLL\n",
    "        criterion_val = end_to_end_loss_NN_NLL_cov\n",
    "        early_stopping = EarlyStopping(patience=10, min_delta=1e-4)\n",
    "        # Freeze weights for coefficient of variation layer before warm-up epochs\n",
    "        warmup = True\n",
    "        for param in model.coe_var_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        train_dataset = train_dataset_CV[fold]\n",
    "        val_dataset = val_dataset_CV[fold]\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch == warmup_epochs:\n",
    "                warmup = False\n",
    "                for param in model.coe_var_layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                inputs, labels = batch\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels, beta, a, warmup, b_weight)\n",
    "                loss_val = criterion_val(outputs, labels, a, warmup, b_weight)\n",
    "                loss.backward()\n",
    "                if epoch >= warmup_epochs:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                train_loss += loss_val.item()\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion_val(outputs, targets, a, warmup, b_weight)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if epoch > warmup_epochs_early_stopping:\n",
    "                early_stopping(val_loss)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        \n",
    "        model_state_dict_10[fold].append(model.state_dict())\n",
    "        val_loss_fold.append(val_loss)\n",
    "        print(f\"Fold {fold + 1}, Model {m + 1}, Validation loss: {val_loss}\")\n",
    "    \n",
    "with open('Best_network/best_NN_NLL_cov_V2_10.pth','wb') as f:\n",
    "    torch.save({'model_state':model_state_dict_10,\n",
    "                'lr':lr,\n",
    "                'num_neuron':best_model_dict['num_neuron'],\n",
    "                'num_layer':best_model_dict['num_layer'],\n",
    "                'batch_size':batch_size,\n",
    "                'weight_decay':weight_decay,\n",
    "                'warmup_epochs':warmup_epochs},f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train_pred_10_all = {}\n",
    "b_test_in_pred_10_all = {}\n",
    "b_test_out_pred_10_all = {}\n",
    "\n",
    "for fold in range(10):\n",
    "    b_train_pred_10 = []\n",
    "    b_test_in_pred_10 = []\n",
    "    b_test_out_pred_10 = []\n",
    "\n",
    "    for m in range(M):\n",
    "        model = Network_cof_variation(input_size, hidden_sizes, output_size)\n",
    "        model.load_state_dict(model_state_dict_10[fold][m])\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            b_train_pred_fold_m = model(X_train_PCA_tensor).numpy()\n",
    "            b_test_in_pred_fold_m = model(X_test_in_PCA_tensor).numpy()\n",
    "            b_test_out_pred_fold_m = model(X_test_out_PCA_tensor).numpy()\n",
    "\n",
    "        b_train_pred_10.append(b_train_pred_fold_m)\n",
    "        b_test_in_pred_10.append(b_test_in_pred_fold_m)\n",
    "        b_test_out_pred_10.append(b_test_out_pred_fold_m)\n",
    "\n",
    "    b_train_pred_10_all[fold] = np.array(b_train_pred_10)\n",
    "    b_test_in_pred_10_all[fold] = np.array(b_test_in_pred_10)\n",
    "    b_test_out_pred_10_all[fold] = np.array(b_test_out_pred_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/b_train_pred_10_all.pkl','wb') as f:\n",
    "    pickle.dump(b_train_pred_10_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/b_test_in_pred_10_all.pkl','wb') as f:\n",
    "    pickle.dump(b_test_in_pred_10_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/b_test_out_pred_10_all.pkl','wb') as f:\n",
    "    pickle.dump(b_test_out_pred_10_all,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_train_pred_10_all = {}\n",
    "Q_test_in_pred_10_all = {}\n",
    "Q_test_out_pred_10_all = {}\n",
    "\n",
    "std_train_pred_10_all = {}\n",
    "std_test_in_pred_10_all = {}\n",
    "std_test_out_pred_10_all = {}\n",
    "\n",
    "Q_train_pred_10_ensemble = {}\n",
    "Q_test_in_pred_10_ensemble = {}\n",
    "Q_test_out_pred_10_ensemble = {}\n",
    "\n",
    "std_train_pred_10_ensemble = {}\n",
    "std_test_in_pred_10_ensemble = {}\n",
    "std_test_out_pred_10_ensemble = {}\n",
    "\n",
    "for fold in range(10):\n",
    "    Q_train_hat_all_models_fold = np.array([[empirical_model(a, b1, b2, b3, N_train[i],b_weight)*100 for i, (b1, b2, b3, _) in enumerate(model_b_train_hat)] for model_b_train_hat in b_train_pred_10_all[fold]])\n",
    "    Q_test_in_hat_all_models_fold = np.array([[empirical_model(a, b1, b2, b3, N_test_in[i],b_weight)*100 for i, (b1, b2, b3, _) in enumerate(model_b_test_in_hat)] for model_b_test_in_hat in b_test_in_pred_10_all[fold]])\n",
    "    Q_test_out_hat_all_models_fold = np.array([[empirical_model(a, b1, b2, b3, N_test_out[i],b_weight)*100 for i, (b1, b2, b3, _) in enumerate(model_b_test_out_hat)] for model_b_test_out_hat in b_test_out_pred_10_all[fold]])\n",
    "\n",
    "    coe_var_train_all_models_fold = np.array([np.repeat(model_b_train_hat[:, 3][:, np.newaxis], Q_train.shape[1], axis=1) for model_b_train_hat in b_train_pred_10_all[fold]])\n",
    "    coe_var_test_in_all_models_fold = np.array([np.repeat(model_b_test_in_hat[:, 3][:, np.newaxis], Q_test_in.shape[1], axis=1) for model_b_test_in_hat in b_test_in_pred_10_all[fold]])\n",
    "    coe_var_test_out_all_models_fold = np.array([np.repeat(model_b_test_out_hat[:, 3][:, np.newaxis], Q_test_out.shape[1], axis=1) for model_b_test_out_hat in b_test_out_pred_10_all[fold]])\n",
    "\n",
    "    # Calculate the variance for each individual model\n",
    "    std_train_all_models_fold = coe_var_train_all_models_fold * (100 - Q_train_hat_all_models_fold)\n",
    "    std_test_in_all_models_fold = coe_var_test_in_all_models_fold * (100 - Q_test_in_hat_all_models_fold)\n",
    "    std_test_out_all_models_fold = coe_var_test_out_all_models_fold * (100 - Q_test_out_hat_all_models_fold)\n",
    "\n",
    "    var_train_all_models_fold = std_train_all_models_fold ** 2\n",
    "    var_test_in_all_models_fold = std_test_in_all_models_fold ** 2\n",
    "    var_test_out_all_models_fold = std_test_out_all_models_fold ** 2\n",
    "\n",
    "    Q_train_hat_combined_fold = np.mean(Q_train_hat_all_models_fold, axis=0)\n",
    "    Q_test_in_hat_combined_fold = np.mean(Q_test_in_hat_all_models_fold, axis=0)\n",
    "    Q_test_out_hat_combined_fold = np.mean(Q_test_out_hat_all_models_fold, axis=0)\n",
    "\n",
    "    # Calculate the variance of the combined predictions\n",
    "    var_train_combined_fold = np.mean(var_train_all_models_fold + Q_train_hat_all_models_fold**2 - Q_train_hat_combined_fold**2, axis=0)\n",
    "    var_test_in_combined_fold = np.mean(var_test_in_all_models_fold + Q_test_in_hat_all_models_fold**2 - Q_test_in_hat_combined_fold**2, axis=0)\n",
    "    var_test_out_combined_fold = np.mean(var_test_out_all_models_fold + Q_test_out_hat_all_models_fold**2 - Q_test_out_hat_combined_fold**2, axis=0)\n",
    "\n",
    "    std_train_combined_fold = np.sqrt(var_train_combined_fold)\n",
    "    std_test_in_combined_fold = np.sqrt(var_test_in_combined_fold)\n",
    "    std_test_out_combined_fold = np.sqrt(var_test_out_combined_fold)\n",
    "\n",
    "    Q_train_pred_10_all[fold] = Q_train_hat_all_models_fold\n",
    "    Q_test_in_pred_10_all[fold] = Q_test_in_hat_all_models_fold\n",
    "    Q_test_out_pred_10_all[fold] = Q_test_out_hat_all_models_fold\n",
    "\n",
    "    std_train_pred_10_all[fold] = std_train_all_models_fold\n",
    "    std_test_in_pred_10_all[fold] = std_test_in_all_models_fold\n",
    "    std_test_out_pred_10_all[fold] = std_test_out_all_models_fold\n",
    "\n",
    "    Q_train_pred_10_ensemble[fold] = Q_train_hat_combined_fold\n",
    "    Q_test_in_pred_10_ensemble[fold] = Q_test_in_hat_combined_fold\n",
    "    Q_test_out_pred_10_ensemble[fold] = Q_test_out_hat_combined_fold\n",
    "\n",
    "    std_train_pred_10_ensemble[fold] = std_train_combined_fold\n",
    "    std_test_in_pred_10_ensemble[fold] = std_test_in_combined_fold\n",
    "    std_test_out_pred_10_ensemble[fold] = std_test_out_combined_fold\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_train_pred_10_all.pkl','wb') as f:\n",
    "    pickle.dump(Q_train_pred_10_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_test_in_pred_10_all.pkl','wb') as f:\n",
    "    pickle.dump(Q_test_in_pred_10_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_test_out_pred_10_all.pkl','wb') as f:\n",
    "    pickle.dump(Q_test_out_pred_10_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_train_pred_10_all.pkl','wb') as f:\n",
    "    pickle.dump(std_train_pred_10_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_test_in_pred_10_all.pkl','wb') as f:\n",
    "    pickle.dump(std_test_in_pred_10_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_test_out_pred_10_all.pkl','wb') as f:\n",
    "    pickle.dump(std_test_out_pred_10_all,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_train_pred_10_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(Q_train_pred_10_ensemble,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_test_in_pred_10_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(Q_test_in_pred_10_ensemble,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/Q_test_out_pred_10_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(Q_test_out_pred_10_ensemble,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_train_pred_10_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(std_train_pred_10_ensemble,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_test_in_pred_10_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(std_test_in_pred_10_ensemble,f)\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/std_test_out_pred_10_ensemble.pkl','wb') as f:\n",
    "    pickle.dump(std_test_out_pred_10_ensemble,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE training: 1.73\n",
      "Mean MAE test in: 2.05\n",
      "Mean MAE test out: 7.14\n",
      "Mean RMSE training: 2.11\n",
      "Mean RMSE test in: 2.47\n",
      "Mean RMSE test out: 8.82\n"
     ]
    }
   ],
   "source": [
    "MAE_training = []\n",
    "MAE_test_in = []\n",
    "MAE_test_out = []\n",
    "\n",
    "RMSE_training = []\n",
    "RMSE_test_in = []\n",
    "RMSE_test_out = []\n",
    "\n",
    "for fold in range(10):\n",
    "    MAE_training_fold = []\n",
    "    MAE_test_in_fold = []\n",
    "    MAE_test_out_fold = []\n",
    "\n",
    "    RMSE_training_fold = []\n",
    "    RMSE_test_in_fold = []\n",
    "    RMSE_test_out_fold = []\n",
    "\n",
    "    for i in range(num_training_cells):\n",
    "        MAE_training_fold.append(mean_absolute_error(Q_train[i]*100,Q_train_pred_10_ensemble[fold][i]))\n",
    "        RMSE_training_fold.append(root_mean_squared_error(Q_train[i]*100,Q_train_pred_10_ensemble[fold][i]))\n",
    "\n",
    "    for i in range(num_test_in_cells):\n",
    "        MAE_test_in_fold.append(mean_absolute_error(Q_test_in[i]*100,Q_test_in_pred_10_ensemble[fold][i]))\n",
    "        RMSE_test_in_fold.append(root_mean_squared_error(Q_test_in[i]*100,Q_test_in_pred_10_ensemble[fold][i]))\n",
    "\n",
    "    for i in range(num_test_out_cells):\n",
    "        MAE_test_out_fold.append(mean_absolute_error(Q_test_out[i]*100,Q_test_out_pred_10_ensemble[fold][i]))\n",
    "        RMSE_test_out_fold.append(root_mean_squared_error(Q_test_out[i]*100,Q_test_out_pred_10_ensemble[fold][i]))\n",
    "\n",
    "    MAE_training.append(np.mean(MAE_training_fold))\n",
    "    MAE_test_in.append(np.mean(MAE_test_in_fold))\n",
    "    MAE_test_out.append(np.mean(MAE_test_out_fold))\n",
    "\n",
    "    RMSE_training.append(np.mean(RMSE_training_fold))\n",
    "    RMSE_test_in.append(np.mean(RMSE_test_in_fold))\n",
    "    RMSE_test_out.append(np.mean(RMSE_test_out_fold))\n",
    "\n",
    "result_dict = {'MAE_training':MAE_training,\n",
    "                'MAE_test_in':MAE_test_in,\n",
    "                'MAE_test_out':MAE_test_out,\n",
    "                'RMSE_training':RMSE_training,\n",
    "                'RMSE_test_in':RMSE_test_in,\n",
    "                'RMSE_test_out':RMSE_test_out}\n",
    "\n",
    "with open('Empirical_parameter_results/E2E_NNE_V2/result_dict_10.pkl','wb') as f:\n",
    "    pickle.dump(result_dict,f)\n",
    "\n",
    "mean_MAE_training = np.mean(MAE_training)\n",
    "mean_MAE_test_in = np.mean(MAE_test_in)\n",
    "mean_MAE_test_out = np.mean(MAE_test_out)\n",
    "\n",
    "mean_RMSE_training = np.mean(RMSE_training)\n",
    "mean_RMSE_test_in = np.mean(RMSE_test_in)\n",
    "mean_RMSE_test_out = np.mean(RMSE_test_out)\n",
    "\n",
    "print(f\"Mean MAE training: {mean_MAE_training:.2f}\")\n",
    "print(f\"Mean MAE test in: {mean_MAE_test_in:.2f}\")\n",
    "print(f\"Mean MAE test out: {mean_MAE_test_out:.2f}\")\n",
    "\n",
    "print(f\"Mean RMSE training: {mean_RMSE_training:.2f}\")\n",
    "print(f\"Mean RMSE test in: {mean_RMSE_test_in:.2f}\")\n",
    "print(f\"Mean RMSE test out: {mean_RMSE_test_out:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE training: [1.784060336859118, 1.8386336529389424, 1.632692972460559, 1.626906289616892, 1.7307085151023, 1.7469161020868753, 1.6925760692281258, 1.703419231528423, 1.7322012733345071, 1.7724808515131016]\n",
      "MAE test in: [2.2784796480191947, 2.1662178337546334, 1.9765103565248399, 1.9790225691551302, 1.7690244412170895, 2.2759907825362307, 1.7942676720690807, 1.9604396249642178, 2.277860275059515, 2.043930906690721]\n",
      "MAE test out: [8.528357093958984, 7.5104533379726846, 8.198342718556477, 6.460058254895792, 3.9085260366471757, 8.791309163180296, 5.630577073762923, 6.4752537960282845, 8.233896754263895, 7.631773181479316]\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAE training: {MAE_training}\")\n",
    "print(f\"MAE test in: {MAE_test_in}\")\n",
    "print(f\"MAE test out: {MAE_test_out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
